<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浩翰Redamancy的博客</title>
  
  <subtitle>一个爱学习的计算机菜鸟笔记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-09-29T15:15:22.981Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>精神小伙</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>修改Compare.py报错时的解决方案</title>
    <link href="http://yoursite.com/2019/09/29/%E4%BF%AE%E6%94%B9Compare-py%E6%8A%A5%E9%94%99%E6%97%B6%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://yoursite.com/2019/09/29/修改Compare-py报错时的解决方案/</id>
    <published>2019-09-29T15:14:49.000Z</published>
    <updated>2019-09-29T15:15:22.981Z</updated>
    
    <content type="html"><![CDATA[<p>错误：<br>在安装opevncv时会出现 ImportError: No module named cv2 的错误，找不到cv2的包。<br>解决：</p><p>这时候安装扩展包即可：</p><p><code>pip install opencv-python</code></p><hr><p>错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;data_generator.py&quot;, line 24, in &lt;module&gt;</span><br><span class="line">    import cv2</span><br><span class="line">  File &quot;/usr/local/lib/python3.5/dist-packages/cv2/__init__.py&quot;, line 3, in &lt;module&gt;</span><br><span class="line">    from .cv2 import *</span><br><span class="line">ImportError: libSM.so.6: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><p>解决方案：<code>sudo apt-get install -y python-qt4</code></p><hr><p>问题：<br><code>ValueError: Input 0 of node Reshape was passed int32 from batch_join:1 incompatible with expected int64.</code></p><p>解决：打开validate_on_lfw.py，找到这三个地方，data_flow_ops.FIFOQueue，labels_placeholder，control_placeholder，将他们的tf.int32全部换成tf.int64重新运行即可</p><hr><p>错误:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SyntaxError:</span><br><span class="line"> Non-UTF-8 code starting with &apos;\xb2&apos; in file src/compare.py on line 58, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details</span><br></pre></td></tr></table></figure></p><p>SyntaxError: Non-ASCII character ‘\xe2’ in file意思是说，在文件中存在非ASCII字符；<br>ASCII是8位即一个字符，一共256个字符，随着计算机的发展，现在已经用到2个或者4个字符；<br>解决方案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">建议在文件头追加：</span><br><span class="line"># -*- coding: cp936 -*-</span><br><span class="line">或者</span><br><span class="line"># -*- coding: utf-8 -*</span><br></pre></td></tr></table></figure><p>文件头追加以后<br>报错：</p><p><code>SyntaxError: (unicode error) &#39;utf-8&#39; codec can&#39;t decode byte 0xb2 in position 0: invalid start byte</code></p><p>解决方案：<br>将该py文件用notepad++打开，转为utf-8编码</p><hr><p><img src="https://img-blog.csdnimg.cn/20190925180841590.png" alt="在这里插入图片描述"><br>原因：<br>excel不能够有效识别出文件中的中文数据<br>解决方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">加入encoding=<span class="string">'utf-8-sig'</span>就不会乱码了</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"result.csv"</span>, <span class="string">"w"</span>,newline=<span class="string">''</span>,encoding=<span class="string">'utf-8-sig'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">        writer = csv.writer(csvfile)</span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">        <span class="comment"># first row</span></span><br><span class="line">        writer.writerow([<span class="string">"查询图像ID"</span>, <span class="string">"t底库中对应top1相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top2相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top3相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top4相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top5相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top6相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top7相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top8相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top9相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top10相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top11相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top12相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top13相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top14相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top15相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top16相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top17相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top18相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top19相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top20相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top21相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top22相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top23相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top24相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top25相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top26相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top27相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top28相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top29相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top30相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top31相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top32相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top33相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top34相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top35相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top36相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top37相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top38相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top39相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top40相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top41相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top42相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top43相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top44相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top45相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top46相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top47相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top48相似度的人脸ID"</span>, <span class="string">"相似度"</span>,</span><br><span class="line">                         <span class="string">"底库中对应top49相似度的人脸ID"</span>, <span class="string">"相似度"</span>, <span class="string">"底库中对应top50相似度的人脸ID"</span>, <span class="string">"相似度"</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">list_result.sort(key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])          <span class="comment"># 根据第2个值排序</span></span><br><span class="line">list_result.sort(key=operator.itemgetter(<span class="number">1</span>)) <span class="comment"># 根据第2个值排序</span></span><br></pre></td></tr></table></figure><p>将元组里的字典按value值排序</p><hr><p>输出形式为列表里的列表元组<img src="https://img-blog.csdnimg.cn/20190925180931462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><code>result = sorted(list_result.items(), key=lambda item: item[1])</code></p><p><img src="https://img-blog.csdnimg.cn/2019092518095460.png" alt="在这里插入图片描述"><br>修改为：</p><pre><code>list_result.extend(images_distance)...list_result=sorted(list_result, key=lambda x:x[1])</code></pre><hr><p>报错：</p><p><code>TypeError: writerow() takes exactly one argument (2 given)</code><br>解决方案<br>将<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"result.csv"</span>, <span class="string">"w"</span>,newline=<span class="string">''</span>,encoding=<span class="string">'utf-8-sig'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">                    writer = csv.writer(csvfile)</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># writerows</span></span><br><span class="line">                    writer.writerow([image_QUERY]+list_result[<span class="number">50</span>])</span><br></pre></td></tr></table></figure></p><p>改变为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"result.csv"</span>, <span class="string">"w"</span>,newline=<span class="string">''</span>,encoding=<span class="string">'utf-8-sig'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">                    writer = csv.writer(csvfile)</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># writerows</span></span><br><span class="line">                    writer.writerow([image_QUERY]+list_result[<span class="number">50</span>])</span><br></pre></td></tr></table></figure></p><hr><p>错误：<br>python提示AttributeError: ‘NoneType’ object has no attribute ‘extend’</p><p>原因：</p><pre><code>a=[]b=[1,2,3,4]a = a.extend(b)</code></pre><p>报出a的类型变为了NoneType<br>extend会修改a本身，并且返回None。不能把返回值再赋值给a。<br>解决：</p><p>将a = a.extend(b)变为 a.extend(b)后问题解决</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;错误：&lt;br&gt;在安装opevncv时会出现 ImportError: No module named cv2 的错误，找不到cv2的包。&lt;br&gt;解决：&lt;/p&gt;
&lt;p&gt;这时候安装扩展包即可：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install opencv-python&lt;/co
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于Facenet与MTCNN的人脸识别</title>
    <link href="http://yoursite.com/2019/09/29/%E5%9F%BA%E4%BA%8EFacenet%E4%B8%8EMTCNN%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2019/09/29/基于Facenet与MTCNN的人脸识别/</id>
    <published>2019-09-29T15:14:15.000Z</published>
    <updated>2019-09-29T15:14:36.190Z</updated>
    
    <content type="html"><![CDATA[<p>本文来自于中国科学院深圳先进技术研究院，目前发表在arXiv上，是2016年4月份的文章，算是比较新的文章。<br>论文地址：<br><a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/" target="_blank" rel="noopener">https://kpzhang93.github.io/MTCNN_face_detection_alignment/</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>相比于R-CNN系列通用检测方法，本文更加针对人脸检测这一专门的任务，速度和精度都有足够的提升。R-CNN，Fast R-CNN，FasterR-CNN这一系列的方法不是一篇博客能讲清楚的，有兴趣可以找相关论文阅读。类似于TCDCN，本文提出了一种Multi-task的人脸检测框架，将人脸检测和人脸特征点检测同时进行。论文使用3个CNN级联的方式，和Viola-Jones类似，实现了coarse-to-fine的算法结构。</p><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>算法流程<br><img src="https://img-blog.csdnimg.cn/20190925171034228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>当给定一张照片的时候，将其缩放到不同尺度形成图像金字塔，以达到尺度不变。</p><p><strong>Stage 1</strong>：使用P-Net是一个全卷积网络，用来生成候选窗和边框回归向量(bounding box regression vectors)。使用Bounding box regression的方法来校正这些候选窗，使用非极大值抑制（NMS）合并重叠的候选框。全卷积网络和Faster R-CNN中的RPN一脉相承。</p><p><strong>Stage 2</strong>：使用N-Net改善候选窗。将通过P-Net的候选窗输入R-Net中，拒绝掉大部分false的窗口，继续使用Bounding box regression和NMS合并。</p><p><strong>Stage 3</strong>：最后使用O-Net输出最终的人脸框和特征点位置。和第二步类似，但是不同的是生成5个特征点位置</p><h3 id="CNN结构"><a href="#CNN结构" class="headerlink" title="CNN结构"></a>CNN结构</h3><p>本文使用三个CNN，结构如图：<br><img src="https://img-blog.csdnimg.cn/20190925171108487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>这个算法需要实现三个任务的学习：人脸非人脸的分类，bounding box regression和人脸特征点定位。</p><p>(1)人脸检测<br>这就是一个分类任务，使用交叉熵损失函数即可：<br>(2)Bounding box regression<br>这是一个回归问题，使用平方和损失函数：<br>(3)人脸特征点定位<br>这也是一个回归问题，目标是5个特征点与标定好的数据的平方和损失：<br>(4)多任务训练<br>不是每个sample都要使用这三种损失函数的，比如对于背景只需要计算，不需要计算别的损失，这样就需要引入一个指示值指示样本是否需要计算某一项损失。最终的训练目标函数是：<br>N是训练样本的数量。表示任务的重要性。在P-Net和R-Net中，在O-Net中，<br>(5)online hard sample mining<br>传统的难例处理方法是检测过一次以后，手动检测哪些困难的样本无法被分类，本文采用online hard sample mining的方法。具体就是在每个mini-batch中，取loss最大的70%进行反向传播，忽略那些简单的样本。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>本文主要使用三个数据集进行训练：FDDB，Wider Face，AFLW。<br>A、训练数据<br>本文将数据分成4种：<br>Negative：非人脸<br>Positive：人脸<br>Part faces：部分人脸<br>Landmark face：标记好特征点的人脸<br>分别用于训练三种不同的任务。Negative和Positive用于人脸分类，positive和part faces用于bounding box regression，landmark face用于特征点定位。<br>B、效果<br>本文的人脸检测和人脸特征点定位的效果都非常好。关键是这个算法速度很快，在2.6GHZ的CPU上达到16fps，在Nvidia Titan达到99fps。</p><p>总结<br>本文使用一种级联的结构进行人脸检测和特征点检测，该方法速度快效果好，可以考虑在移动设备上使用。这种方法也是一种由粗到细的方法，和Viola-Jones的级联AdaBoost思路相似。<br>类似于Viola-Jones：1、如何选择待检测区域：图像金字塔+P-Net；2、如何提取目标特征：CNN；3、如何判断是不是指定目标：级联判断。</p><h2 id="本次实验过程"><a href="#本次实验过程" class="headerlink" title="本次实验过程"></a>本次实验过程</h2><p>使用MTCNN将人脸选择出来（分割人脸），然后使用facenet训练（欧氏距离算法</p><h3 id="compare-py源码如下"><a href="#compare-py源码如下" class="headerlink" title="compare.py源码如下"></a>compare.py源码如下</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Performs face alignment and calculates L2 distance between the embeddings of images."""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MIT License</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Copyright (c) 2016 David Sandberg</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># Permission is hereby granted, free of charge, to any person obtaining a copy</span></span><br><span class="line"><span class="comment"># of this software and associated documentation files (the "Software"), to deal</span></span><br><span class="line"><span class="comment"># in the Software without restriction, including without limitation the rights</span></span><br><span class="line"><span class="comment"># to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span></span><br><span class="line"><span class="comment"># copies of the Software, and to permit persons to whom the Software is</span></span><br><span class="line"><span class="comment"># furnished to do so, subject to the following conditions:</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># The above copyright notice and this permission notice shall be included in all</span></span><br><span class="line"><span class="comment"># copies or substantial portions of the Software.</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span></span><br><span class="line"><span class="comment"># IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span></span><br><span class="line"><span class="comment"># FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span></span><br><span class="line"><span class="comment"># AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span></span><br><span class="line"><span class="comment"># LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span></span><br><span class="line"><span class="comment"># OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span></span><br><span class="line"><span class="comment"># SOFTWARE.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> misc</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> facenet</span><br><span class="line"><span class="keyword">import</span> align.detect_face</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#使用MTCNN网络在原始图片中进行检测和对齐</span></span><br><span class="line">    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">      </span><br><span class="line">            <span class="comment"># Load the facenet  model</span></span><br><span class="line">            facenet.load_model(args.model)</span><br><span class="line">    </span><br><span class="line">            <span class="comment"># Get input and output tensors</span></span><br><span class="line">            <span class="comment"># 输入图像占位符</span></span><br><span class="line">            images_placeholder = tf.get_default_graph().get_tensor_by_name(<span class="string">"input:0"</span>)</span><br><span class="line">            <span class="comment">#卷及网络最后输出的"特征"</span></span><br><span class="line">            embeddings = tf.get_default_graph().get_tensor_by_name(<span class="string">"embeddings:0"</span>)</span><br><span class="line">            <span class="comment">#训练？</span></span><br><span class="line">            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(<span class="string">"phase_train:0"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Run forward pass to calculate embeddings </span></span><br><span class="line">            feed_dict = &#123; images_placeholder: images, phase_train_placeholder:<span class="literal">False</span> &#125;</span><br><span class="line">            emb = sess.run(embeddings, feed_dict=feed_dict)</span><br><span class="line">            </span><br><span class="line">            nrof_images = len(args.image_files)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'Images:'</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">                print(<span class="string">'%1d: %s'</span> % (i, args.image_files[i]))</span><br><span class="line">            print(<span class="string">''</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Print distance matrix</span></span><br><span class="line">            print(<span class="string">'Distance matrix'</span>)</span><br><span class="line">            print(<span class="string">'    '</span>, end=<span class="string">''</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">                print(<span class="string">'    %1d     '</span> % i, end=<span class="string">''</span>)</span><br><span class="line">            print(<span class="string">''</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">                print(<span class="string">'%1d  '</span> % i, end=<span class="string">''</span>)</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">                    <span class="comment">#对特征计算两两之间的距离以得到人脸之间的相似度</span></span><br><span class="line">                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))</span><br><span class="line">                    print(<span class="string">'  %1.4f  '</span> % dist, end=<span class="string">''</span>)</span><br><span class="line">                print(<span class="string">''</span>)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_and_align_data</span><span class="params">(image_paths, image_size, margin, gpu_memory_fraction)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    返回经过MTCNN处理后的人脸图像集合 [n,160,160,3]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    minsize = <span class="number">20</span> <span class="comment"># minimum size of face</span></span><br><span class="line">    threshold = [ <span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.7</span> ]  <span class="comment"># three steps's threshold</span></span><br><span class="line">    factor = <span class="number">0.709</span> <span class="comment"># scale factor</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#创建P-Net,R-Net,O-Net网络，并加载参数</span></span><br><span class="line">    print(<span class="string">'Creating networks and loading parameters'</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)</span><br><span class="line">        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=<span class="literal">False</span>))</span><br><span class="line">        <span class="keyword">with</span> sess.as_default():</span><br><span class="line">            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, <span class="literal">None</span>)</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line">    tmp_image_paths=copy.copy(image_paths)</span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment">#遍历测试图片</span></span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> tmp_image_paths:</span><br><span class="line">        img = misc.imread(os.path.expanduser(image), mode=<span class="string">'RGB'</span>)</span><br><span class="line">        img_size = np.asarray(img.shape)[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        <span class="comment">#人脸检测 bounding_boxes：表示边界框 形状为[n,5] 5对应x1,y1,x2,y2,score</span></span><br><span class="line">        <span class="comment">#_：人脸关键点坐标 形状为 [n,10]</span></span><br><span class="line">        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)</span><br><span class="line">        <span class="keyword">if</span> len(bounding_boxes) &lt; <span class="number">1</span>:</span><br><span class="line">          image_paths.remove(image)</span><br><span class="line">          print(<span class="string">"can't detect face, remove "</span>, image)</span><br><span class="line">          <span class="keyword">continue</span></span><br><span class="line">        <span class="comment">#对图像进行处理:扩展、裁切、缩放</span></span><br><span class="line">        det = np.squeeze(bounding_boxes[<span class="number">0</span>,<span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line">        bb = np.zeros(<span class="number">4</span>, dtype=np.int32)</span><br><span class="line">        bb[<span class="number">0</span>] = np.maximum(det[<span class="number">0</span>]-margin/<span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">        bb[<span class="number">1</span>] = np.maximum(det[<span class="number">1</span>]-margin/<span class="number">2</span>, <span class="number">0</span>)</span><br><span class="line">        bb[<span class="number">2</span>] = np.minimum(det[<span class="number">2</span>]+margin/<span class="number">2</span>, img_size[<span class="number">1</span>])</span><br><span class="line">        bb[<span class="number">3</span>] = np.minimum(det[<span class="number">3</span>]+margin/<span class="number">2</span>, img_size[<span class="number">0</span>])</span><br><span class="line">        cropped = img[bb[<span class="number">1</span>]:bb[<span class="number">3</span>],bb[<span class="number">0</span>]:bb[<span class="number">2</span>],:]</span><br><span class="line">        aligned = misc.imresize(cropped, (image_size, image_size), interp=<span class="string">'bilinear'</span>)</span><br><span class="line">        <span class="comment">#归一化处理</span></span><br><span class="line">        prewhitened = facenet.prewhiten(aligned)</span><br><span class="line">        img_list.append(prewhitened)</span><br><span class="line">    <span class="comment">#[n,160,160,3]</span></span><br><span class="line">    images = np.stack(img_list)</span><br><span class="line">    <span class="keyword">return</span> images</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_arguments</span><span class="params">(argv)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    参数解析</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">'model'</span>, type=str, </span><br><span class="line">        help=<span class="string">'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'image_files'</span>, type=str, nargs=<span class="string">'+'</span>, help=<span class="string">'Images to compare'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--image_size'</span>, type=int,</span><br><span class="line">        help=<span class="string">'Image size (height, width) in pixels.'</span>, default=<span class="number">160</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--margin'</span>, type=int,</span><br><span class="line">        help=<span class="string">'Margin for the crop around the bounding box (height, width) in pixels.'</span>, default=<span class="number">44</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--gpu_memory_fraction'</span>, type=float,</span><br><span class="line">        help=<span class="string">'Upper bound on the amount of GPU memory that will be used by the process.'</span>, default=<span class="number">1.0</span>)</span><br><span class="line">    <span class="keyword">return</span> parser.parse_args(argv)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(parse_arguments(sys.argv[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure><p><code>compare.py</code> 该py文件作用是用自己的图像上应用已有模型来计算人脸之间的距离，即欧氏距离。<br>当欧氏距离小于1时，我们可以看做输入的两个照片为同一个人。</p><p>这三张照片分为起名为 img1.jpg,img2.jpg,img3.jpg。<br><img src="https://img-blog.csdnimg.cn/20190925171501754.png" alt="在这里插入图片描述"><br>在facenet目录下运行 </p><p><code>python src/compare.py models/20180408-102900 src/img1.jpg  src/img2.jpg src/img3.jpg</code></p><p>结果：<br><img src="https://img-blog.csdnimg.cn/20190925171536641.png" alt="在这里插入图片描述"><br>上面为官方的输入和输出</p><p>但是在该比赛中，比赛评委要求输出：</p><pre><code>（输出）检索结果：要求参评单位将检索结果整理为CSV文件。每一项用制表符&apos;\t&apos;分割，每一行具体格式如下：（所有输出以UTF-8无BOM格式编码）查询图像ID\t底库中对应top1相似度的人脸ID\t相似度\t底库中对应top2相似度的人脸ID\t相似度\t…\t底库中对应top50相似度的人脸ID\t相似度</code></pre><p>我尝试用compare.py文件的欧氏距离来计算出图片的相似度</p><h2 id="开始修改源码"><a href="#开始修改源码" class="headerlink" title="开始修改源码"></a>开始修改源码</h2><p>（过程很痛苦，网上没有修改compare.py文件的例子）</p><p>1.将矩阵输出为1行，即只取第一行矩阵。因为比赛只要求用QUERY _查询集唯一ID.jpg去DB库里比较所有图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Distance matrix'</span>)</span><br><span class="line">print(<span class="string">'    '</span>, end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">    print(<span class="string">'    %1d     '</span> % i, end=<span class="string">''</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'%1d  '</span> % <span class="number">0</span>, end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">    dist = np.sqrt(np.sum(np.square(np.subtract(emb[<span class="number">0</span>, :], emb[j, :]))))   <span class="comment">#计算欧式距离</span></span><br><span class="line">    print(<span class="string">'  %1.4f  '</span> % dist, end=<span class="string">''</span>)</span><br><span class="line">    list.append(dist)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line">print(list[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190925171840227.png" alt="在这里插入图片描述"></p><ol start="2"><li>将图片id与矩阵里的数据组成字典，并且按照比赛要求排序</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">list_images=[]  <span class="comment">#图片路径的列表</span></span><br><span class="line">print(<span class="string">'Images:'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">    print(<span class="string">'%1d: %s'</span> % (i, image_files[i]))</span><br><span class="line">    list_images.append(image_files[i])<span class="comment">#图片路径存入</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Distance matrix'</span>)</span><br><span class="line">print(<span class="string">'    '</span>, end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">    print(<span class="string">'    %1d     '</span> % i, end=<span class="string">''</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'%1d  '</span> % <span class="number">0</span>, end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">list_distance = []<span class="comment">#欧式距离的列表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(nrof_images):</span><br><span class="line">    dist = np.sqrt(np.sum(np.square(np.subtract(emb[<span class="number">0</span>, :], emb[j, :]))))</span><br><span class="line">    print(<span class="string">'  %1.4f  '</span> % dist, end=<span class="string">''</span>)</span><br><span class="line">    list_distance.append(dist)<span class="comment">#欧氏距离存入</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line">print(list_distance[<span class="number">1</span>:])</span><br><span class="line">images_distance=dict(zip(list_images[<span class="number">1</span>:], list_distance[<span class="number">1</span>:]))<span class="comment">#用dict方法将list_images和list_distance存入字典</span></span><br><span class="line">result=sorted(images_distance.items(),key=<span class="keyword">lambda</span> item:item[<span class="number">1</span>])<span class="comment">#排序</span></span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>知识点：</p><p><strong>sorted函数</strong></p><p>sorted(iterable,key,reverse)，sorted一共有iterable,key,reverse这三个参数;<br>其中iterable表示可以迭代的对象，例如可以是dict.items()、dict.keys()等<br>key是一个函数，用来选取参与比较的元素，reverse则是用来指定排序是倒序还是顺序，reverse=true则是倒序，<br>reverse=false时则是顺序，默认时reverse=false。</p><p><strong>dict(zip)</strong><br>zip是Python中的一个内建函数，能够用来组合多个序列类型的数据。它会把传入的所有序列中下标相同的元素组成一个个元组，以最短的序列为基准。</p><p>3.用QUERY里的照片去DB库找相似度前50的人脸照片，需要知道DB库中所有照片的路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">images_paths=glob.glob(<span class="string">r"./QUERY/raw/*/*.jpg"</span>)</span><br></pre></td></tr></table></figure><p>4.用for循环将QUERY _查询集唯一ID.jpg与DB库中的照片每10张就比一次</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#set a b</span><br><span class="line">a = 0</span><br><span class="line">b = 9</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line"></span><br><span class="line">    image_files = image_QUERY + images_DB[a:b]</span><br><span class="line"></span><br><span class="line">    if b &lt;= 20:</span><br><span class="line"></span><br><span class="line">        images = load_and_align_data(image_files, image_size, margin, gpu_memory_fraction)</span><br><span class="line"></span><br><span class="line">        images_placeholder = tf.get_default_graph().get_tensor_by_name(&quot;input:0&quot;)</span><br><span class="line"></span><br><span class="line">        embeddings = tf.get_default_graph().get_tensor_by_name(&quot;embeddings:0&quot;)</span><br><span class="line"></span><br><span class="line">        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(&quot;phase_train:0&quot;)</span><br><span class="line"></span><br><span class="line">        feed_dict = &#123;images_placeholder: images, phase_train_placeholder: False&#125;</span><br><span class="line">        emb = sess.run(embeddings, feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">        nrof_images = len(image_files)</span><br><span class="line"></span><br><span class="line">        list_images=[]</span><br><span class="line">        print(&apos;Images:&apos;)</span><br><span class="line">        for i in range(nrof_images):</span><br><span class="line">            print(&apos;%1d: %s&apos; % (i, image_files[i]))</span><br><span class="line">            list_images.append(image_files[i])</span><br><span class="line">        print(&apos;&apos;)</span><br><span class="line"></span><br><span class="line">        print(&apos;Distance matrix&apos;)</span><br><span class="line">        print(&apos;    &apos;, end=&apos;&apos;)</span><br><span class="line"></span><br><span class="line">        for i in range(nrof_images):</span><br><span class="line">            print(&apos;    %1d     &apos; % i, end=&apos;&apos;)</span><br><span class="line">        print(&apos;&apos;)</span><br><span class="line"></span><br><span class="line">        print(&apos;%1d  &apos; % 0, end=&apos;&apos;)</span><br><span class="line"></span><br><span class="line">        list_distance = []</span><br><span class="line"></span><br><span class="line">        for j in range(nrof_images):</span><br><span class="line">            dist = np.sqrt(np.sum(np.square(np.subtract(emb[0, :], emb[j, :]))))</span><br><span class="line">            print(&apos;  %1.4f  &apos; % dist, end=&apos;&apos;)</span><br><span class="line">            list_distance.append(dist)</span><br><span class="line">        print(&apos;&apos;)</span><br><span class="line">        print(list_distance[1:])</span><br><span class="line">        images_distance=dict(zip(list_images[1:], list_distance[1:]))</span><br><span class="line">        result=sorted(images_distance.items(),key=lambda item:item[1])</span><br><span class="line">        print(result)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">    a += 10</span><br><span class="line">    b += 10</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190925172250628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="5"><li>将数据写入列表，然后排出前50，并且写入csv文件</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"result.csv"</span>,<span class="string">"w"</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    writer = csv.writer(csvfile)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#先写入columns_name</span></span><br><span class="line">    writer.writerow([<span class="string">"查询图像ID"</span>,<span class="string">"t底库中对应top1相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top2相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top3相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top4相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top5相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top6相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top7相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top8相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top9相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top10相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top11相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top12相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top13相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top14相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top15相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top16相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top17相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top18相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top19相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top20相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top21相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top22相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top23相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top24相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top25相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top26相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top27相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top28相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top29相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top30相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top31相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top32相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top33相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top34相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top35相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top36相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top37相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top38相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top39相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top40相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top41相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top42相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top43相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top44相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top45相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top46相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top47相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top48相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top49相似度的人脸ID"</span>,<span class="string">"相似度"</span>,<span class="string">"底库中对应top50相似度的人脸ID"</span>,<span class="string">"相似度"</span>])</span><br><span class="line">    <span class="comment">#写入多行用writerows</span></span><br><span class="line">    writer.writerows([image_QUERY],result[<span class="number">50</span>])</span><br></pre></td></tr></table></figure><p>1）将QUERY的照片一个一个与DB里所有的照片进行比对</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">image_QUERY_path = glob.glob(<span class="string">r"./QUERY/*.jpg"</span>)</span><br><span class="line">image_QUERY = image_QUERY_path</span><br><span class="line">i=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> image_QUERY <span class="keyword">in</span> image_QUERY[i]:</span><br><span class="line">    ...</span><br><span class="line">i+=<span class="number">1</span></span><br></pre></td></tr></table></figure><p>将列表里的元组用逗号分开，并且消除括号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list_result=&apos;,&apos;.join(list_result[0:50])</span><br></pre></td></tr></table></figure></p><p>报错：<br><img src="https://img-blog.csdnimg.cn/20190925180412645.png" alt="在这里插入图片描述"><br>原因：<br>list包含数字，不能直接转化成字符串。<br>解决办法：<code>&#39;,&#39;.join(&#39;%s&#39; %result for result in list_result)</code><br>即遍历list的元素，把它转化成字符串<br>运行后报错：<br><code>TypeError: not all arguments converted during string formatting</code></p><p>原因: % 操作符只能直接用于字符串(‘123’)，列表([1,2,3])、元组</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文来自于中国科学院深圳先进技术研究院，目前发表在arXiv上，是2016年4月份的文章，算是比较新的文章。&lt;br&gt;论文地址：&lt;br&gt;&lt;a href=&quot;https://kpzhang93.github.io/MTCNN_face_detection_alignment/&quot; 
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python 字典列表相互转换与排序</title>
    <link href="http://yoursite.com/2019/09/29/python-%E5%AD%97%E5%85%B8%E5%88%97%E8%A1%A8%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2%E4%B8%8E%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2019/09/29/python-字典列表相互转换与排序/</id>
    <published>2019-09-29T15:13:46.000Z</published>
    <updated>2019-09-29T15:14:03.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="列表-元组-字典-集合的区别"><a href="#列表-元组-字典-集合的区别" class="headerlink" title="列表 元组 字典 集合的区别"></a>列表 元组 字典 集合的区别</h2><p>列表：清单可重复，类型可不同 list<br>元组: 类似列表不可修改类型  tuple<br>集合：就是我们数学学的集合应用是去重 set<br>字典：字典存储键值对数据价值是查询，通过键，查找值 dict</p><h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><p>列表的特点：可重复，类型可不同，这是与数组最本质的区别。<br>python中的列表用“[]”表示</p><p>list=[‘asd’,123]</p><p>向list中添加项有两种方法：append和extend。append<br>使用append可能会出现下面的这种情况<br><img src="https://img-blog.csdnimg.cn/20190925165015933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>列表里会叠加列表<br>这是使用extend就不会出现这种问题</p><h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><p>元组和列表在结构上没有什么区别，唯一的差异在于元组是只读的，不能修改。<br>元组用“()”表示，如：<br>tup =(‘asd’,123)<br>元组可以使用切片，例如tup[0]</p><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>集合就是我们数学学的集合，没有什么特殊的定义。集合最好的应用是去重。<br>list = [ 1, 11, 1]<br>list_set = set( list )  #lst_set 为1 , 11</p><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><p>最后一个是字典。字典存储键值对数据，{key:value}，如：<br>{1:a,2:b,3:c}<br>      字典最外面用大括号，每一组用冒号连起来，然后各组用逗号隔开。<br>　   字典最大的价值是查询，通过key，查找value。</p><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><h3 id="字典-1"><a href="#字典-1" class="headerlink" title="字典"></a>字典</h3><p>dict = {‘a’: ‘1’, ‘b’: 2, ‘c’: ‘3’}<br>将该字典转换为字符串<br>str(dict)<br>返回：<br> {‘a’: ‘1’, ‘b’: 2, ‘c’: ‘3’}（注：这里虽然看着一样，但其实字典已经变为字符串，可以使用type(dict)查看）<br>将该字典转为元组<br>tuple(dict)<br>返回：<br>(‘a’,’b’,’c’)<br>将该字典values值转为元组<br>tuple(dict.values())<br>返回:<br>(‘1’,’2’,’3’)<br>将该字典转为列表<br>(list(dict))<br>[‘a’,’b’,’c’]</p><h3 id="元组-1"><a href="#元组-1" class="headerlink" title="元组"></a>元组</h3><p>tup=(1, 2, 3, 4, 5)<br>将元组转为字符串，返回：(1, 2, 3, 4, 5)<br>(tup.<strong>str</strong>()<br>将元组转为列表，返回：[1, 2, 3, 4, 5]<br>list(tup)<br>元组不可以转为字典</p><h3 id="列表-1"><a href="#列表-1" class="headerlink" title="列表"></a>列表</h3><p>nums=[1, 3, 5, 7, 8, 13, 20]<br>将列表转为字符串，返回：[1, 3, 5, 7, 8, 13, 20]<br>str(nums)<br>将列表转为元组，返回：(1, 3, 5, 7, 8, 13, 20)<br>tuple(nums)<br>列表不可以转为字典</p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>将字符串转为元组，返回：(1, 2, 3)<br>tuple(eval(“(1,2,3)”))<br>将字符串转为列表，返回：[1, 2, 3]<br>list(eval(“(1,2,3)”))</p><p>#字符串转为字典，返回：<type 'dict'><br>type(eval(“{‘name’:’ljq’, ‘age’:24}”))</type></p><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="字典-2"><a href="#字典-2" class="headerlink" title="字典"></a>字典</h3><p>创建一个字典</p><p>dict1={‘a’:2,’b’:3,’c’:8,’d’:4}</p><p>1、取字典的所有键，所有的值，利用dict1.keys()，dict1.vaules()，</p><p>由于键，值有很多个，所以要加s，另外注意这里要加括号，这样的小细节不注意，很容易犯错。</p><p>结果：<br>dict_values([4, 2, 8, 3]) dict_keys([‘d’, ‘a’, ‘c’, ‘b’])<br>可以看出，返回的是列表的形式</p><p>2、同时取字典的键、值，dict1.items()，这里同样加s和括号</p><p>print(dict1.items())<br>结果：</p><p>dict_items([(‘d’, 4), (‘a’, 2), (‘c’, 8), (‘b’, 3)])<br>可以看出，返回的结果是元组组成的列表</p><p>也就是说，通过dict1.items()这个函数，把字典形式的键、值，存在了一个元组内。</p><p>3、对字典进行排序</p><p>3.1 先看一下，直接用sorted()排序的情况。</p><p>dict1={‘a’:2,’e’:3,’f’:8,’d’:4}<br>dict2 = sorted(dict1)<br>print(dict2)<br>结果：</p><p>[‘a’, ‘d’, ‘e’, ‘f’]<br>sorted()默认是对字典的键，从小到大进行排序</p><p>3.2 、对键值进行反向（从大到小）排序</p><p>dict1={‘a’:2,’e’:3,’f’:8,’d’:4}<br>dict2 = sorted(dict1,reverse=True)<br>print(dict2)<br>结果：[‘f’, ‘e’, ‘d’, ‘a’]<br>像这种对键进行排序，往往是为了得到 值（value）</p><p>拿到键最大，对应的值，如：</p><p>print(dict1[dict2[0]])#结果为8<br>当然我们也可以先拿到所有的key，然后再对key排序</p><p>dict1={‘a’:2,’e’:3,’f’:8,’d’:4}<br>list1= sorted(dict1.keys(),reverse=True)<br>print(list1)    # 结果：[‘f’, ‘e’, ‘d’, ‘a’]<br>3.3、对value进行排序</p><p>同样，用dict1.values()得到所有的values，然后对value排序</p><p>dict1={‘a’:2,’e’:3,’f’:8,’d’:4}<br>list1= sorted(dict1.values())<br>print(list1)    #结果：[2, 3, 4, 8]<br>设值reverse=True 进行反向排序</p><p>也可以用dict1.items()，得到包含键，值的元组</p><p>由于迭代对象是元组，返回值自然是元组组成的列表</p><p>这里对排序的规则进行了定义，x指元组，x[1]是值，x[0]是键</p><p>dict1={‘a’:2,’e’:3,’f’:8,’d’:4}<br>list1= sorted(dict1.items(),key=lambda x:x[1])<br>print(list1)<br>结果：</p><p>[(‘a’, 2), (‘e’, 3), (‘d’, 4), (‘f’, 8)]<br>对键进行排序：</p><p>dict1={‘a’:2,’e’:3,’f’:8,’d’:4}<br>list1= sorted(dict1.items(),key=lambda x:x[0])<br>print(list1)<br>结果：</p><p>[(‘a’, 2), (‘d’, 4), (‘e’, 3), (‘f’, 8)]</p><h3 id="列表-2"><a href="#列表-2" class="headerlink" title="列表"></a>列表</h3><p>基本的列表排序</p><p>1.list.sort()排序</p><p>data = [5, 7, 9, 3, -6, -7, -8, -9, 3, -8]<br>result = data.sort()<br>print(data) #结果为 [-9, -8, -8, -7, -6, 3, 3, 5, 7, 9]<br>print(result) #结果为None<br>1<br>2<br>3<br>4<br>2.sorted()排序</p><p>data = [5, 7, 9, 3, -6, -7, -8, -9, 3, -8]<br>result = sorted(data)<br>print(data) #结果为 [5, 7, 9, 3, -6, -7, -8, -9, 3, -8]<br>print(result) #结果为 [-9, -8, -8, -7, -6, 3, 3, 5, 7, 9]</p><p>当元组最为list的元素时</p><p>在默认情况下sort和sorted函数接收的参数是元组时，它将会先按元组的第一个元素进行排序再按第二个元素进行排序，再按第三个、第四个…依次排序。<br>我们通过一个简单的例子来了解它，以下面这个list为例：<br><code>data = [(1, &#39;B&#39;), (1, &#39;A&#39;), (2, &#39;A&#39;), (0, &#39;B&#39;), (0, &#39;a&#39;)]</code><br>我们通过sorted()对它进行排序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = [(1, &apos;B&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (0, &apos;a&apos;)]</span><br><span class="line">result = sorted(data)</span><br><span class="line">print(data) #结果为 [(1, &apos;B&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (0, &apos;a&apos;)]</span><br><span class="line">print(result) #结果为 [(0, &apos;B&apos;), (0, &apos;a&apos;), (1, &apos;A&apos;), (1, &apos;B&apos;), (2, &apos;A&apos;)]</span><br></pre></td></tr></table></figure></p><p>会发现排序后的结果中(0, ‘B’)在(0, ‘a’)的前面。这是因为在按元组第一个元素排好之后，将(0, ‘B’), (0, ‘a’)再按第二个元素进行排序了，而’B’的ASCII编码比’a’小，所以(0, ‘B’)就排在(0, ‘a’)的前面了。</p><p>那如何想要让它排序时不分大小写呢？</p><p>这就要用到sort方法和sorted方法里的key参数了。<br>我们来看一下具体的实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [(1, &apos;B&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (0, &apos;a&apos;)]</span><br><span class="line">#利用参数key来规定排序的规则</span><br><span class="line">result = sorted(data,key=lambda x:(x[0],x[1].lower()))</span><br><span class="line"></span><br><span class="line">print(data) #结果为 [(1, &apos;B&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (0, &apos;a&apos;)]</span><br><span class="line">print(result) #结果为 [(0, &apos;a&apos;), (0, &apos;B&apos;), (1, &apos;A&apos;), (1, &apos;B&apos;), (2, &apos;A&apos;)]</span><br></pre></td></tr></table></figure></p><p>其中的lambda x:(x[0],x[1].lower())可以理解为一个匿名函数；<br>其功能类似于:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def fun(x)</span><br><span class="line">    return(x[0],x[1].lower())</span><br></pre></td></tr></table></figure></p><p>如果想要以字母作为第一排序规则,并且字母大小写不敏感，该怎么实现？<br>这就能要运用到之前所讲到的</p><p>在默认情况下sort和sorted函数接收的参数是元组时，它将会先按元组的第一个元素进行排序再按第二个元素进行排序，再按第三个、第四个…依次排序。</p><p>再配合lambda返回一个自定义tuple；代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [(1, &apos;B&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (0, &apos;a&apos;)]</span><br><span class="line">将x[1].lower()作为返回元组里的第一个元素,按照sorted的排序规律,就会先按字母排序,再按数字排序了</span><br><span class="line">result = sorted(data,key=lambda x:(x[1].lower(),x[0]))</span><br><span class="line"></span><br><span class="line">print(data) #结果为 [(1, &apos;B&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (0, &apos;a&apos;)] </span><br><span class="line">print(result) #结果为 [(0, &apos;a&apos;), (1, &apos;A&apos;), (2, &apos;A&apos;), (0, &apos;B&apos;), (1, &apos;B&apos;)]</span><br></pre></td></tr></table></figure></p><h3 id="当字典作为列表的元素时"><a href="#当字典作为列表的元素时" class="headerlink" title="当字典作为列表的元素时"></a>当字典作为列表的元素时</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = [&#123;&apos;name&apos;: &apos;张三&apos;, &apos;height&apos;: 175&#125;, &#123;&apos;name&apos;: &apos;李四&apos;, &apos;height&apos;: 165&#125;, &#123;&apos;name&apos;: &apos;王五&apos;, &apos;height&apos;: 185&#125;]</span><br><span class="line">#将x[&apos;height&apos;]最为返回tuple的第个一元素</span><br><span class="line">result = sorted(data,key=lambda x:(x[&apos;height&apos;],x[&apos;name&apos;]))</span><br><span class="line">print(data) #结果为 </span><br><span class="line">print(result)</span><br><span class="line">#data   结果:[&#123;&apos;name&apos;: &apos;张三&apos;, &apos;height&apos;: 175&#125;, &#123;&apos;name&apos;: &apos;李四&apos;, &apos;height&apos;: 165&#125;, &#123;&apos;name&apos;: &apos;王五&apos;, &apos;height&apos;: 185&#125;]</span><br><span class="line">#result 结果:[&#123;&apos;name&apos;: &apos;李四&apos;, &apos;height&apos;: 165&#125;, &#123;&apos;name&apos;: &apos;张三&apos;, &apos;height&apos;: 175&#125;, &#123;&apos;name&apos;: &apos;王五&apos;, &apos;height&apos;: 185&#125;]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;列表-元组-字典-集合的区别&quot;&gt;&lt;a href=&quot;#列表-元组-字典-集合的区别&quot; class=&quot;headerlink&quot; title=&quot;列表 元组 字典 集合的区别&quot;&gt;&lt;/a&gt;列表 元组 字典 集合的区别&lt;/h2&gt;&lt;p&gt;列表：清单可重复，类型可不同 list&lt;br
      
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>python 读写csv数据</title>
    <link href="http://yoursite.com/2019/09/29/python-%E8%AF%BB%E5%86%99csv%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2019/09/29/python-读写csv数据/</id>
    <published>2019-09-29T15:13:17.000Z</published>
    <updated>2019-09-29T15:13:35.374Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CSV文件简介"><a href="#CSV文件简介" class="headerlink" title="CSV文件简介"></a>CSV文件简介</h2><p>CSV(Comma-Separated Values，逗号分隔值)，是一种纯文本形式存储表格数据的文件。该文件由任意数目的记录组成，每条记录被分隔符分隔为字段（最常见的分隔符是逗号或制表符），且每条记录都有相同的字段序列，因此csv相当于一个结构化表的纯文本形式。从直观上看，它比Excel文件更加简洁，然而它不包含诸如XLS电子表格的数值、公式和格式等内容，它仅仅为一个结构化的纯文本。</p><h2 id="CSV文件读取和写入"><a href="#CSV文件读取和写入" class="headerlink" title="CSV文件读取和写入"></a>CSV文件读取和写入</h2><p>在CSV文件读写操作中，常用两种读写方式是列表读写和字典读写，下面我们分别来对此进行介绍。</p><h3 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h3><p>csv文件读取主要是使用reader()和DictReader()方法，二者均接收一个csv文件参数，并返回一个用于文件读取迭代器。这两个方法的区别是：reader()方法获取的是一行行列表数据的迭代器，每行的数据可通过下标来获取，而DictReader()方法获取的是一行行字典数据的迭代器，每行的数据可通过键来获取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##CSV文件读取的两种方式</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列表读取</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    reader = csv.reader(fp)   <span class="comment">#返回读取迭代器</span></span><br><span class="line">    titles = next(reader)     <span class="comment">#提取出文件记录标题</span></span><br><span class="line">    print(type(titles))       <span class="comment">#&lt;class 'list'&gt;</span></span><br><span class="line">    print(titles)             <span class="comment">#['id', 'name', 'city']</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> reader:          <span class="comment">#遍历向下迭代</span></span><br><span class="line">        print(x)              <span class="comment">#['001', 'Mike', 'Beijing']...</span></span><br><span class="line">        id = x[<span class="number">0</span>]</span><br><span class="line">        name = x[<span class="number">1</span>]</span><br><span class="line">        city = x[<span class="number">2</span>]</span><br><span class="line">        print(&#123;<span class="string">'id'</span>: id, <span class="string">'name'</span>: name, <span class="string">'city'</span>: city&#125;)    <span class="comment">#&#123;'id': '001', 'name': 'Mike', 'city': 'Beijing'&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字典读取</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'data.csv'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    reader = csv.DictReader(fp)     <span class="comment">#迭代器，但不包含标题数据(第0行)</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> reader:</span><br><span class="line">        print(type(x))              <span class="comment">#&lt;class 'collections.OrderedDict'&gt;</span></span><br><span class="line">        print(x)                    <span class="comment">#OrderedDict([('id', '001'), ('name', 'Mike'), ('city', 'Beijing')])...</span></span><br><span class="line">        id = x[<span class="string">'id'</span>]</span><br><span class="line">        name = x[<span class="string">'name'</span>]</span><br><span class="line">        city = x[<span class="string">'city'</span>]</span><br><span class="line">        print(&#123;<span class="string">'id'</span>: id, <span class="string">'name'</span>: name, <span class="string">'city'</span>: city&#125;)    <span class="comment">#&#123;'id': '001', 'name': 'Mike', 'city': 'Beijing'&#125;</span></span><br></pre></td></tr></table></figure><h3 id="文件写入"><a href="#文件写入" class="headerlink" title="文件写入"></a>文件写入</h3><p>同文件读取一样，文件的写入也有两种方法——writer()和DictWriter()，其含义和reader()/DictReader()相类似，writer()用于列表数据写入，而DictWriter()用于字典数据写入。二者使用方法也比较简单，但需要注意的是由于是写入文件，需要指明文件的编码方式(特别是需要写入中文字符时)，具体的用法如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##CSV文件写入的两种方式</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列表写入</span></span><br><span class="line"><span class="comment"># 设置记录标题(列表)和记录值(一个嵌套元组集或列表集的列表)</span></span><br><span class="line">headers = [<span class="string">'id'</span>,<span class="string">'name'</span>,<span class="string">'province'</span>]</span><br><span class="line">values = [</span><br><span class="line">    (<span class="string">'001'</span>,<span class="string">'ShenZhen'</span>,<span class="string">'GuangDong'</span>),</span><br><span class="line">    (<span class="string">'002'</span>, <span class="string">'WuHan'</span>, <span class="string">'HuBei'</span>),</span><br><span class="line">    (<span class="string">'003'</span>, <span class="string">'ChengDu'</span>, <span class="string">'SiChuan'</span>)</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 使用open函数时设置参数encoding以防止乱码</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'citylist.csv'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>,newline=<span class="string">''</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    writer = csv.writer(fp)    <span class="comment">#获取文件</span></span><br><span class="line">    writer.writerow(headers)   <span class="comment">#写入一行记录</span></span><br><span class="line">    writer.writerows(values)   <span class="comment">#写入多行记录，传入的参数为列表结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字典写入</span></span><br><span class="line"><span class="comment"># 设置记录标题(列表)和记录值(一个嵌套字典集的列表)</span></span><br><span class="line">headers = [<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'province'</span>]</span><br><span class="line">values = [</span><br><span class="line">    &#123;<span class="string">'id'</span>: <span class="string">'001'</span>, <span class="string">'name'</span>: <span class="string">'ShenZhen'</span>, <span class="string">'province'</span>: <span class="string">'GuangDong'</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'id'</span>: <span class="string">'002'</span>, <span class="string">'name'</span>: <span class="string">'WuHan'</span>, <span class="string">'province'</span>: <span class="string">'HuBei'</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'id'</span>: <span class="string">'003'</span>, <span class="string">'name'</span>: <span class="string">'ChengDu'</span>, <span class="string">'province'</span>: <span class="string">'SiChuan'</span>&#125;</span><br><span class="line">]</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'citydict.csv'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>, newline=<span class="string">''</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    writer = csv.DictWriter(fp,headers)    <span class="comment">#获取文件，注意参数还需传递记录标题以映射，注意此时并不会真正写入标题</span></span><br><span class="line">    writer.writeheader()                   <span class="comment">#写入记录标题</span></span><br><span class="line">    writer.writerows(values)               <span class="comment">#写入多行记录</span></span><br></pre></td></tr></table></figure><p>在打开待写入CSV文件时，这里我们还传入了一个newline参数，并且其值为空字符串，这么做是为了防止在每次写完一行后其会自动再写入一个换行符，如下图为设置和不设置newline的文件写入对应结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">设置了newline的文件写入结果：</span><br><span class="line">“””</span><br><span class="line">id,name,province</span><br><span class="line">001,ShenZhen,GuangDong</span><br><span class="line">002,WuHan,HuBei</span><br><span class="line">003,ChengDu,SiChuan</span><br><span class="line">“””</span><br><span class="line">未设置newline的文件写入结果：</span><br><span class="line">“””</span><br><span class="line">id,name,province</span><br><span class="line"> </span><br><span class="line">001,ShenZhen,GuangDong</span><br><span class="line"> </span><br><span class="line">002,WuHan,HuBei</span><br><span class="line"> </span><br><span class="line">003,ChengDu,SiChuan</span><br><span class="line"> </span><br><span class="line">“””</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;CSV文件简介&quot;&gt;&lt;a href=&quot;#CSV文件简介&quot; class=&quot;headerlink&quot; title=&quot;CSV文件简介&quot;&gt;&lt;/a&gt;CSV文件简介&lt;/h2&gt;&lt;p&gt;CSV(Comma-Separated Values，逗号分隔值)，是一种纯文本形式存储表格数据的文
      
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>在Ubuntu的Anaconda环境下创建深度学习环境</title>
    <link href="http://yoursite.com/2019/09/29/%E5%9C%A8Ubuntu%E7%9A%84Anaconda%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%88%9B%E5%BB%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    <id>http://yoursite.com/2019/09/29/在Ubuntu的Anaconda环境下创建深度学习环境/</id>
    <published>2019-09-29T15:12:50.000Z</published>
    <updated>2019-09-29T15:13:05.053Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>首先在所在系统中安装Anaconda。可以打开命令行输入conda -V检验是否安装以及当前conda的版本<br>conda版本为4.5.11</p><p><img src="https://img-blog.csdnimg.cn/20190925163611459.png" alt="在这里插入图片描述"><br><strong>conda常用的命令</strong></p><ol><li><code>conda list</code> 查看安装了哪些包<br><img src="https://img-blog.csdnimg.cn/20190925163712758.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li><li><code>conda env list</code>或 <code>conda info -e</code> 查看当前存在哪些虚拟环境<br><img src="https://img-blog.csdnimg.cn/2019092516375094.png" alt="在这里插入图片描述"><ol start="3"><li>conda update conda 检查更新当前conda</li></ol></li></ol><h3 id="创建python虚拟环境"><a href="#创建python虚拟环境" class="headerlink" title="创建python虚拟环境"></a>创建python虚拟环境</h3><p>使用 conda create -n your_env_name python=X.X（2.7、3.6等)命令创建python版本为X.X、名字为your_env_name的虚拟环境。your_env_name文件可以在Anaconda安装目录envs文件下找到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/cuda/.conda/envs/comp_face_recognition/lib/python3.6/site-packages</span><br></pre></td></tr></table></figure><p>创建了一个名为comp_face_recognition python版本为3.6的环境</p><p><code>conda create -n comp_face_recognition pip python=3.6</code></p><p><img src="https://img-blog.csdnimg.cn/20190925163915330.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="在虚拟环境中安装cuda和cudnn"><a href="#在虚拟环境中安装cuda和cudnn" class="headerlink" title="在虚拟环境中安装cuda和cudnn"></a>在虚拟环境中安装cuda和cudnn</h3><p>注意：当你的系统中已经安装了cuda和cudnn这一步就不需要了，虚拟环境中需要使用的cuda和cudnn会自动调用系统中的。但是如果你的系统的cudnn和cuda版本和所要安装的tensorflow或者pytorch不匹配，就需要在虚拟环境中安装。</p><p>由于该系统中已经安装了cuda和cudnn，所以不需要安装这些</p><h3 id="使用激活-或切换不同python版本-的虚拟环境"><a href="#使用激活-或切换不同python版本-的虚拟环境" class="headerlink" title="使用激活(或切换不同python版本)的虚拟环境"></a>使用激活(或切换不同python版本)的虚拟环境</h3><p>打开命令行输入python –version可以检查当前python的版本。</p><pre><code>使用如下命令即可 激活你的虚拟环境(即将python的版本改变)。Linux:  source activate your_env_name(虚拟环境名称)Windows: activate your_env_name(虚拟环境名称)</code></pre><p>  这是再使用python –version可以检查当前python版本是否为想要的<br><code>conda activate comp_face_recognition</code><br><img src="https://img-blog.csdnimg.cn/2019092516401230.png" alt="在这里插入图片描述"></p><h3 id="安装TensorFlow-Gpu与keras"><a href="#安装TensorFlow-Gpu与keras" class="headerlink" title="安装TensorFlow-Gpu与keras"></a>安装TensorFlow-Gpu与keras</h3><p><code>conda install tensorflow-gpu keras</code></p><p><code>conda list</code><br><img src="https://img-blog.csdnimg.cn/20190925164036932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="关闭虚拟环境-即从当前环境退出返回使用PATH环境中的默认python版本"><a href="#关闭虚拟环境-即从当前环境退出返回使用PATH环境中的默认python版本" class="headerlink" title="关闭虚拟环境(即从当前环境退出返回使用PATH环境中的默认python版本)"></a>关闭虚拟环境(即从当前环境退出返回使用PATH环境中的默认python版本)</h3><p>  使用如下命令即可。</p><pre><code>Linux: source deactivateWindows: deactivate</code></pre><h3 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a>删除虚拟环境</h3><p>   使用命令<code>conda remove -n your_env_name(虚拟环境名称) --all</code>， 即可删除。</p><h3 id="删除环境中的某个包"><a href="#删除环境中的某个包" class="headerlink" title="删除环境中的某个包"></a>删除环境中的某个包</h3><p>   使用命令<code>conda remove --name your_env_name  package_name</code> 即可</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;p&gt;首先在所在系统中安装Anaconda。可以打开命令行输入conda -V检验是否安装以及当前conda的版本&lt;br&gt;cond
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>centos7 访问php 该网页无法正常运作 目前无法处理此请求</title>
    <link href="http://yoursite.com/2019/09/29/centos7-%E8%AE%BF%E9%97%AEphp-%E8%AF%A5%E7%BD%91%E9%A1%B5%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E8%BF%90%E4%BD%9C-%E7%9B%AE%E5%89%8D%E6%97%A0%E6%B3%95%E5%A4%84%E7%90%86%E6%AD%A4%E8%AF%B7%E6%B1%82/"/>
    <id>http://yoursite.com/2019/09/29/centos7-访问php-该网页无法正常运作-目前无法处理此请求/</id>
    <published>2019-09-29T15:11:43.000Z</published>
    <updated>2019-09-29T15:12:31.616Z</updated>
    
    <content type="html"><![CDATA[<p>修改php.ini文件<br>文件在<code>/etc/php.ini</code></p><p>由于php.ini配置文件中错误显示关闭导致.<br>将下值由Off 变更为 On<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">display_errors = On</span><br><span class="line">display_startup_errors = On</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;修改php.ini文件&lt;br&gt;文件在&lt;code&gt;/etc/php.ini&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;由于php.ini配置文件中错误显示关闭导致.&lt;br&gt;将下值由Off 变更为 On&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;t
      
    
    </summary>
    
      <category term="Liunx" scheme="http://yoursite.com/categories/Liunx/"/>
    
    
      <category term="PHP" scheme="http://yoursite.com/tags/PHP/"/>
    
  </entry>
  
  <entry>
    <title>分布式消息系统Kafka</title>
    <link href="http://yoursite.com/2019/09/29/%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E7%B3%BB%E7%BB%9FKafka/"/>
    <id>http://yoursite.com/2019/09/29/分布式消息系统Kafka/</id>
    <published>2019-09-29T15:11:05.000Z</published>
    <updated>2019-09-29T15:11:28.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分布式消息系统Kafka"><a href="#分布式消息系统Kafka" class="headerlink" title="分布式消息系统Kafka"></a>分布式消息系统Kafka</h1><p>Kafka可以处理消费者规模的网站中的所有动作流数据。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息</p><p>kafka是一个分布式的、可分区的、可复制的消息系统；</p><p>kafka是由LinkedIn开发，使用Scala编写；</p><p>支持水平拓展和高吞吐率；</p><p>可与Apache Storm、Spark等多种开源分布式处理系统集成。</p><h2 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识:"></a>相关知识:</h2><p>（1）以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能<br>（2）高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输<br>（3）支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输<br>（4）同时支持离线数据处理和实时数据处理<br>（5）Scale out：支持在线水平扩展<br>Kafka中各个组件的功能：<br>（1）Broker： Kafka集群包含一个或多个服务器，这种服务器被称为broker<br>（2）Topic：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上，但用户只需指定消息的Topic即可生产或消费数据，不必关心数据存于何处）<br>（3）Partition：Parition是物理上的概念，每个Topic包含一个或多个Partition<br>（4）Producer：负责发布消息到Kafka broker<br>（5）Consumer：消息消费者，向Kafka broker读取消息的客户端<br>（6）Consumer Group：每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）</p><h2 id="Kafka应用场景"><a href="#Kafka应用场景" class="headerlink" title="Kafka应用场景"></a>Kafka应用场景</h2><p>活动数据包括页面访问量、被查看内容方面的信息以及搜索情况等内容<br>构建应用系统和分析系统的桥梁，并将它们之间的关联解耦；<br>支持近实时的在线分析系统和类似于Hadoop之类的离线分析系统；<br>具有高可扩展性。即：当数据量增加时，可以通过增加节点进行水平扩展。</p><h2 id="Kafka与Flume"><a href="#Kafka与Flume" class="headerlink" title="Kafka与Flume"></a>Kafka与Flume</h2><p><img src="https://img-blog.csdnimg.cn/20190918211151591.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Kafka与Flume联用"><a href="#Kafka与Flume联用" class="headerlink" title="Kafka与Flume联用"></a>Kafka与Flume联用</h2><p><img src="https://img-blog.csdnimg.cn/20190918211211321.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Kafka的模型"><a href="#Kafka的模型" class="headerlink" title="Kafka的模型"></a>Kafka的模型</h2><p><img src="https://img-blog.csdnimg.cn/20190918211231876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Kafka的名词说明"><a href="#Kafka的名词说明" class="headerlink" title="Kafka的名词说明"></a>Kafka的名词说明</h2><p><img src="https://img-blog.csdnimg.cn/20190918211257634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="生产者-消费者模式"><a href="#生产者-消费者模式" class="headerlink" title="生产者/消费者模式"></a>生产者/消费者模式</h2><p><img src="https://img-blog.csdnimg.cn/20190918211308167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="发布-订阅模式"><a href="#发布-订阅模式" class="headerlink" title="发布/订阅模式"></a>发布/订阅模式</h2><p><img src="https://img-blog.csdnimg.cn/20190918211331492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="https://img-blog.csdnimg.cn/20190918211345337.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Push和Pull（拉数据）<br>Consumer自己选择拉数据</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p><img src="https://img-blog.csdnimg.cn/20190918211406734.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="环境部署"><a href="#环境部署" class="headerlink" title="环境部署"></a>环境部署</h2><p><img src="https://img-blog.csdnimg.cn/20190918211421457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190918211430563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190918211440934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;分布式消息系统Kafka&quot;&gt;&lt;a href=&quot;#分布式消息系统Kafka&quot; class=&quot;headerlink&quot; title=&quot;分布式消息系统Kafka&quot;&gt;&lt;/a&gt;分布式消息系统Kafka&lt;/h1&gt;&lt;p&gt;Kafka可以处理消费者规模的网站中的所有动作流数据。 对于
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Kafka" scheme="http://yoursite.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>ZooKeeper学习笔记</title>
    <link href="http://yoursite.com/2019/09/29/ZooKeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/09/29/ZooKeeper学习笔记/</id>
    <published>2019-09-29T15:10:30.000Z</published>
    <updated>2019-09-29T15:10:54.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-什么是-ZooKeeper"><a href="#一-什么是-ZooKeeper" class="headerlink" title="一 什么是 ZooKeeper"></a>一 什么是 ZooKeeper</h1><h2 id="ZooKeeper-的由来"><a href="#ZooKeeper-的由来" class="headerlink" title="ZooKeeper 的由来"></a>ZooKeeper 的由来</h2><p>Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。</p><p>关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的Pig项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家RaghuRamakrishnan开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好要用来进行分布式环境的协调一一于是，Zookeeper的名字也就由此诞生了。</p><blockquote><p>内容摘自《从Paxos到Zookeeper 》第四章第一节</p></blockquote><h2 id="ZooKeeper-概览"><a href="#ZooKeeper-概览" class="headerlink" title="ZooKeeper 概览"></a>ZooKeeper 概览</h2><p>ZooKeeper 是一个开源的分布式协调服务，ZooKeeper框架最初是在“Yahoo!”上构建的，用于以简单而稳健的方式访问他们的应用程序。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。</p><p>原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。</p><p>ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。</p><p>Zookeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心。 服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。如下图所示，在 Dubbo架构中 Zookeeper 就担任了注册中心这一角色。</p><h2 id="结合个人使用情况的讲一下-ZooKeeper"><a href="#结合个人使用情况的讲一下-ZooKeeper" class="headerlink" title="结合个人使用情况的讲一下 ZooKeeper"></a>结合个人使用情况的讲一下 ZooKeeper</h2><p>在我自己做过的项目中，主要使用到了 ZooKeeper 作为 Dubbo 的注册中心(Dubbo 官方推荐使用 ZooKeeper注册中心)。另外在搭建 solr 集群的时候，我使用 ZooKeeper 作为 solr 集群的管理工具。这时，ZooKeeper 主要提供下面几个功能：1、集群管理：容错、负载均衡。2、配置文件的集中管理3、集群的入口。<br>我个人觉得在使用 ZooKeeper 的时候，最好是使用 集群版的 ZooKeeper 而不是单机版的。官网给出的架构图就描述的是一个集群版的 ZooKeeper 。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。</p><p>为什么最好使用奇数台服务器构成 ZooKeeper 集群？</p><p>我们知道在Zookeeper中 Leader 选举算法采用了Zab协议。Zab核心思想是当多数 Server 写成功，则任务数据写成功。<br>①如果有3个Server，则最多允许1个Server 挂掉。<br>②如果有4个Server，则同样最多允许1个Server挂掉。<br>既然3个或者4个Server，同样最多允许1个Server挂掉，那么它们的可靠性是一样的，所以选择奇数个ZooKeeper Server即可，这里选择3个Server。12341234</p><h2 id="关于-ZooKeeper-的一些重要概念"><a href="#关于-ZooKeeper-的一些重要概念" class="headerlink" title="关于 ZooKeeper 的一些重要概念"></a>关于 ZooKeeper 的一些重要概念</h2><h3 id="重要概念总结"><a href="#重要概念总结" class="headerlink" title="重要概念总结"></a>重要概念总结</h3><p>ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。</p><p>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。</p><p>ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。</p><p>ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）</p><p>ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。</p><p>ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提交数据节点监听服务。</p><p>下面关于会话（Session）、 Znode、版本、Watcher、ACL概念的总结都在《从Paxos到Zookeeper 》第四章第一节以及第七章第八节有提到，感兴趣的可以看看！</p><h3 id="会话（Session）"><a href="#会话（Session）" class="headerlink" title="会话（Session）"></a>会话（Session）</h3><p>Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。</p><p>在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。</p><h3 id="Znode"><a href="#Znode" class="headerlink" title="Znode"></a>Znode</h3><p>在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。</p><p>Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。</p><p>在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL.一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。</p><h3 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h3><p>在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat中记录了这个 ZNode 的三个数据版本，分别是version（当前ZNode的版本）、cversion（当前ZNode子节点的版本）和 cversion（当前ZNode的ACL版本）。</p><h3 id="Watcher"><a href="#Watcher" class="headerlink" title="Watcher"></a>Watcher</h3><p>Watcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。</p><h3 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h3><p>Zookeeper采用ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。<br><img src="https://img-blog.csdnimg.cn/20190918210828982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制。</p><h2 id="ZooKeeper-特点"><a href="#ZooKeeper-特点" class="headerlink" title="ZooKeeper 特点"></a>ZooKeeper 特点</h2><p>顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。</p><p>原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。</p><p>单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。</p><p>可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。</p><p>四 ZooKeeper 设计目标</p><h2 id="简单的数据模型"><a href="#简单的数据模型" class="headerlink" title="简单的数据模型"></a>简单的数据模型</h2><p>ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。</p><p><img src="https://img-blog.csdnimg.cn/20190918210848500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="可构建集群"><a href="#可构建集群" class="headerlink" title="可构建集群"></a>可构建集群</h2><p>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。<br>ZooKeeper 官方提供的架构图：</p><p><img src="https://img-blog.csdnimg.cn/20190918210906519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>上图中每一个Server代表一个安装Zookeeper服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。</p><h3 id="顺序访问"><a href="#顺序访问" class="headerlink" title="顺序访问"></a>顺序访问</h3><p>对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id）</p><h3 id="高性能"><a href="#高性能" class="headerlink" title="高性能"></a>高性能</h3><p>ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）</p><h2 id="ZooKeeper-集群角色介绍"><a href="#ZooKeeper-集群角色介绍" class="headerlink" title="ZooKeeper 集群角色介绍"></a>ZooKeeper 集群角色介绍</h2><p>最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。</p><p>但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示</p><p><img src="https://img-blog.csdnimg.cn/20190918210920322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。</p><p><img src="https://img-blog.csdnimg.cn/20190918210942763.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="ZooKeeper-amp-ZAB-协议-amp-Paxos算法"><a href="#ZooKeeper-amp-ZAB-协议-amp-Paxos算法" class="headerlink" title="ZooKeeper &amp;ZAB 协议&amp;Paxos算法"></a>ZooKeeper &amp;ZAB 协议&amp;Paxos算法</h2><h3 id="ZAB-协议-amp-Paxos算法"><a href="#ZAB-协议-amp-Paxos算法" class="headerlink" title="ZAB 协议&amp;Paxos算法"></a>ZAB 协议&amp;Paxos算法</h3><p>Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在ZooKeeper的官方文档中也指出，ZAB协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播算法。</p><h3 id="ZAB-协议介绍"><a href="#ZAB-协议介绍" class="headerlink" title="ZAB 协议介绍"></a>ZAB 协议介绍</h3><p>ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。</p><h3 id="ZAB-协议两种基本的模式：崩溃恢复和消息广播"><a href="#ZAB-协议两种基本的模式：崩溃恢复和消息广播" class="headerlink" title="ZAB 协议两种基本的模式：崩溃恢复和消息广播"></a>ZAB 协议两种基本的模式：崩溃恢复和消息广播</h3><p>ZAB协议包括两种基本的模式，分别是 崩溃恢复和消息广播。当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进人恢复模式并选举产生新的Leader服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和Leader服务器的数据状态保持一致。</p><p>当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。 当一台同样遵守ZAB协议的服务器启动后加人到集群中时，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么新加人的服务器就会自觉地进人数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。正如上文介绍中所说的，ZooKeeper设计成只允许唯一的一个Leader服务器来进行事务请求的处理。Leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议；而如果集群中的其他机器接收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过阅读本文，想必大家已从 ①ZooKeeper的由来。 -&gt; ②ZooKeeper 到底是什么 。-&gt; ③ ZooKeeper 的一些重要概念（会话（Session）、 Znode、版本、Watcher、ACL）-&gt; ④ZooKeeper 的特点。 -&gt; ⑤ZooKeeper 的设计目标。-&gt; ⑥ ZooKeeper 集群角色介绍 （Leader、Follower 和 Observer 三种角色）-&gt; ⑦ZooKeeper &amp;ZAB 协议&amp;Paxos算法。 这七点了解了 ZooKeeper 。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一-什么是-ZooKeeper&quot;&gt;&lt;a href=&quot;#一-什么是-ZooKeeper&quot; class=&quot;headerlink&quot; title=&quot;一 什么是 ZooKeeper&quot;&gt;&lt;/a&gt;一 什么是 ZooKeeper&lt;/h1&gt;&lt;h2 id=&quot;ZooKeeper-的由来
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="ZooKeeper" scheme="http://yoursite.com/tags/ZooKeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper环境搭建</title>
    <link href="http://yoursite.com/2019/09/29/Zookeeper%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/09/29/Zookeeper环境搭建/</id>
    <published>2019-09-29T15:09:58.000Z</published>
    <updated>2019-09-29T15:10:14.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="zookeeper集群安装"><a href="#zookeeper集群安装" class="headerlink" title="zookeeper集群安装"></a>zookeeper集群安装</h2><p><strong>在master，slave1，slave2上安装</strong></p><p><strong>hadoop用户进入master</strong></p><p><code>cd /opt/hadoop/</code></p><p><code>tar -zxvf zookeeper-3.4.8.tar.gz</code></p><p><code>vim /etc/profile</code></p><pre><code>#zookeeperexport ZOOKEEPER_HOME=/opt/hadoop/zookeeper-3.4.8export PATH=$ZOOKEEPER_HOME/bin:$PATH</code></pre><hr><pre><code>cd zookeeper-3.4.8/confcp zoo_sample.cfg zoo.cfg</code></pre><p>修改配置文件 zoo.cfg</p><pre><code># The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/opt/hadoop/zookeeper-3.4.8/datadataLogDir=/opt/hadoop/zookeeper-3.4.8/logs# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60# Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance# The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1#写入节点ip与端口server.4=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888</code></pre><p>保存退出</p><pre><code>cd /opt/hadoop/zookeeper-3.4.8mkdir datamkdir logscd datavim myid4保存退出</code></pre><p>将zookeeper文件传到salve1与slave2中</p><pre><code>scp -r zooker-3.4.8 root@slave1:/opt/hadoop/scp -r zooker-3.4.8 root@slave2:/opt/hadoop/</code></pre><p>在salve1与slave2中分别将myid修改为2 3 </p><p>在bin目录下</p><pre><code>./zkServer.sh start./zkServer.sh status</code></pre><hr><pre><code>./zkServer.sh start-foregroundzookeeper启动不了可以使用该命令查看报错</code></pre><p><img src="https://img-blog.csdnimg.cn/20190918203802544.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190918203818791.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/20190918203828217.png" alt="在这里插入图片描述"></p><h2 id="问题：为什么master不是leader"><a href="#问题：为什么master不是leader" class="headerlink" title="问题：为什么master不是leader"></a>问题：为什么master不是leader</h2><p>先以一个简单的例子来说明zookeeper整个选举的过程：</p><p>假设有五台服务器组成的zookeeper集群,它们的id从1-5（对应的是zoo.cfg中的server.serverID值，同时对应myid文件的值）<br><img src="https://img-blog.csdnimg.cn/20190918203855267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.<br>1) 服务器1启动,此时只有它一台服务器启动了,它发出去的信息没有任何响应,所以它的选举状态一直是LOOKING状态<br>2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.</p><p>注：每台zookeeper服务器启动时都是先选举自己然后再去对比ID值</p><p>3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.<br>4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.<br>5) 服务器5启动,同4一样,当小弟.</p><p>总结：从以上的例子可以看出leader的选举不仅跟ID的大小有关系，还跟启动的顺序有关系，因为ID的值越大而且还第一个启动，那它就可以被选举为leader。</p><p>现在大家就应该清楚如何自定义leader了，只要把你想要自定义为leader机器的ID值设置比其他的值都要大，并且先启动你要自定义leader的机器，这样就可以自定义成功了。</p><p>自定义过程中有可能会出现的问题：</p><p>按照以上的做法去操作但是想要自定义的机器没有变为leader却是follower，原因是因为在你启动自定义机器时它还没有完全启动成功就启动了其他的机器，这样等自定义机器启动完之后其余的机器已经选好了leader，所以就只能成为follower。又或者自定义机器启动失败也会造成以上原因</p><p>将master的值修改为4后<br><img src="https://img-blog.csdnimg.cn/20190918203925384.png" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;zookeeper集群安装&quot;&gt;&lt;a href=&quot;#zookeeper集群安装&quot; class=&quot;headerlink&quot; title=&quot;zookeeper集群安装&quot;&gt;&lt;/a&gt;zookeeper集群安装&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;在master，slave1，sla
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Zookeeper" scheme="http://yoursite.com/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop学习笔记</title>
    <link href="http://yoursite.com/2019/09/29/Sqoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/09/29/Sqoop学习笔记/</id>
    <published>2019-09-29T15:09:15.000Z</published>
    <updated>2019-09-29T15:09:41.922Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据转移工具Sqoop"><a href="#数据转移工具Sqoop" class="headerlink" title="数据转移工具Sqoop"></a>数据转移工具Sqoop</h2><p>Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Apache框架Hadoop是一个越来越通用的分布式计算环境，主要用来处理大数据。随着云提供商利用这个框架，更多的用户将数据集在Hadoop和传统数据库之间转移，Sqoop这个帮助数据传输的工具变得更加重要。</p><p><img src="https://img-blog.csdnimg.cn/20190918202837182.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(Mysql、Oracle…)间进行数据的传递，可以将一个关系型数据库中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。</p><p><strong>特点：</strong></p><ul><li>可导入单个或者所有数据库中的表格</li><li>可以通过WHERE指定导入的行 列</li><li>可以提供任意的SELECT语句</li><li>可以自动生成一个Hive表格, 根据输入的数据</li><li>可以支持增长性的数据导入</li><li>可以将HDFS导出到其他数据库</li><li>支持文本文件(–as-textfile)、avro(–as-avrodatafile)、SequenceFiles(–as-sequencefile)。</li></ul><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p><strong>Sqoop检查每一个table并且自动生成一个Java class来导入数据到HDFS</strong></p><p>Sqoop会产生和运行一个Map-only的MapReduce job来导入数据</p><p><code>默认会有4个Mapper连接到RDBMS, 每一个导入1/4数据</code><br><img src="https://img-blog.csdnimg.cn/20190918203137452.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="Sqoop-Connectors"><a href="#Sqoop-Connectors" class="headerlink" title="Sqoop Connectors"></a>Sqoop Connectors</h3><p>自定义Sqoop connectors提供更高速的访问;<br>目前支持Netezza, Teradata, Oracle Database</p><h2 id="Sqoop2："><a href="#Sqoop2：" class="headerlink" title="Sqoop2："></a>Sqoop2：</h2><p><img src="https://img-blog.csdnimg.cn/20190918203213985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="sqoop和sqoop2的区别"><a href="#sqoop和sqoop2的区别" class="headerlink" title="sqoop和sqoop2的区别:"></a>sqoop和sqoop2的区别:</h2><p><img src="https://img-blog.csdnimg.cn/20190918203244206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><pre><code>当HDFS有数据写入时, NameNode会把文件标记为存在, 但是size = 0当每一份block写入后, 其他的clients会看到block为了避免多人同时访问同一份数据, 最好是先将数据导入到一个临时目录;当文件完全写入后, 将其一直目标文件夹(atomic操作), 因为这个操作只需要NameNode更新一下metaData, 所以也很快;一些机构标准的注释方式:./incoming/..../for_processing/..../completed/...</code></pre><p>REST接口也可以访问HDFS<br>WebHDFS, HttpFS</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据转移工具Sqoop&quot;&gt;&lt;a href=&quot;#数据转移工具Sqoop&quot; class=&quot;headerlink&quot; title=&quot;数据转移工具Sqoop&quot;&gt;&lt;/a&gt;数据转移工具Sqoop&lt;/h2&gt;&lt;p&gt;Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Sqoop" scheme="http://yoursite.com/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Flume学习笔记</title>
    <link href="http://yoursite.com/2019/09/29/Flume%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/09/29/Flume学习笔记/</id>
    <published>2019-09-29T15:08:31.000Z</published>
    <updated>2019-09-29T15:08:56.396Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么用Flume："><a href="#为什么用Flume：" class="headerlink" title="为什么用Flume："></a>为什么用Flume：</h2><p><img src="https://img-blog.csdnimg.cn/20190918202234865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="可靠性："><a href="#可靠性：" class="headerlink" title="可靠性："></a>可靠性：</h3><p><img src="https://img-blog.csdnimg.cn/20190918202256888.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Flume是Cloudera公司的一款高性能、高可用的分布式日志收集系统。</p><p>Flume的核心是把数据从数据源收集过来再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，再删除缓存的数据。</p><p>Flume传输数据的基本单位是event，如果是文本文件，通常是一行记录，这也是事务的基本单位。</p><p>Flume运行的核心是Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是Source、Channel、Sink。</p><p><img src="https://img-blog.csdnimg.cn/20190918202319133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol><li><p>Source组件是专门用于收集日志的，可以处理各种类型各种格式的日志数据，包括Avro、Thrift、Exec、Jms、Spooling directory、Netcat、Sequence Generator、Syslog、HTTP、Legacy、自定义。</p></li><li><p>Source组件把数据收集来以后，临时存放在Channel中。</p></li><li><p>Channel组件是在Agent中专门用于临时存储数据的，Source收集的数据将临时储蓄于此，可以存放在Memory、Jdbc、File、自定义。</p></li><li><p>Channel中的数据只有在Sink发送成功之后才会被删除。</p></li><li><p>Sink组件是用于把Channel中数据发送到目的地的组件，目的地包括HDFS、Logger、Avro、Thrift、Ipc、File、Null、HBase、Solr、自定义。</p></li></ol><p>在整个数据传输过程中，流动的是event。事务保证是在event级别。</p><p>Flume配置：Source、Channel、Sink</p><p>Source的类型主要有：Exec、Avro、Netcat、Spooldir、 Http 、Syslogtcp 、Seq、Thrift等。</p><p><img src="https://img-blog.csdnimg.cn/20190918202357364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Channel的类型主要有File、 Memory 、JDBC等。</p><p><img src="https://img-blog.csdnimg.cn/20190918202410988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Sink的类型主要有：Null、HDFS、 HBase、 Hive、Thrift、 Avro、Logger等。</p><p><img src="https://img-blog.csdnimg.cn/20190918202451290.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>执行代码：<br>下面我们令Source为exec类型，搭配Channel的memory或file类型，Sink的logger或hdfs类型进行Flume配置实验。</p><p><img src="https://img-blog.csdnimg.cn/2019091820250514.png" alt="在这里插入图片描述"><br>实验一：exec_mem_logger.conf</p><pre><code>#定义各个组件agent1.sources  = srcagent1.channels = chagent1.sinks    = des#配置sourceagent1.sources.src.type = execagent1.sources.src.command = tail -n 20 /data/flume2/goods#配置channelagent1.channels.ch.type = memory#配置sinkagent1.sinks.des.type = logger##下面是把上面设置的组件关联起来（把点用线连起来）agent1.sources.src.channels = chagent1.sinks.des.channel    = ch</code></pre><p>启动flume命令：</p><p><code>flume-ng agent -c /conf -f /apps/flume/conf/exec_mem_logger.conf -n agent1 -Dflume.root.logger=DEBUG,console</code></p><p>参数说明：</p><p>#source:exec、channel:memory、 sink:logger<br>-c 配置文件存放的目录<br>-f 所使用的配置文件路径<br>-n agent的名称</p><p>开启flume后，查看输出效果<br><img src="https://img-blog.csdnimg.cn/20190918202555606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>source:exec，channel:memory，sink:hdfs。 相对于上一个实验，它的Sink类型发生了变化，变成了hdfs型。其结构中定义的各组件，Source配置没有变，在配置Channel时最大容量capacity为100000，通信的最大容量为100，在配置Sink时类型变为hdfs，/%Y/%m/%d，里面的%Y/%m/%d代表年月日，数据类型为文本型，写入格式为Text格式，写入hdfs的文件是否新建有几种判断方式：rollInterval表示基于时间判断，单位是秒，当为0时，表示不基于时间判断。rollSize表示基于文件大小判断，单位是B，当为0时表示不基于大小判断，rollCount表示基于写入记录的条数来判断，当为0时，表示不基于条数来判断。idleTimeout表示基于空闲时间来判断，单位是秒，当为0时，代表不基于空闲时间来判断。最后上次一样通过设置Source和Sink的Channel都为ch，把Source、Channel和Sink三个组件关联起来。</p><p>实验二：exec_mem_hdfs.conf</p><pre><code>#定义各个组件agent1.sources  = srcagent1.channels = chagent1.sinks    = des#配置sourceagent1.sources.src.type = execagent1.sources.src.command = tail -n 20 /data/flume2/goods#配置channelagent1.channels.ch.type = memoryagent1.channels.ch.keep-alive = 30agnet1.channels.ch.capacity = 1000000agent1.channels.ch.transactionCapacity = 100#配置sinkagent1.sinks.des.type = hdfsagent1.sinks.des.hdfs.path = hdfs://localhost:9000/myflume2/exec_mem_hdfs/%Y%m%d/agent1.sinks.des.hdfs.useLocalTimeStamp = true#设置flume临时文件的前缀为 . 或 _ 在hive加载时，会忽略此文件。agent1.sinks.des.hdfs.inUsePrefix=_#设置flume写入文件的前缀是什么agent1.sinks.des.hdfs.filePrefix = abcagent1.sinks.des.hdfs.fileType = DataStreamagent1.sinks.des.hdfs.writeFormat = Text#hdfs创建多久会新建一个文件，0为不基于时间判断,单位为秒agent1.sinks.des.hdfs.rollInterval = 30#hdfs写入的文件达到多大时，创建新文件 0为不基于空间大小,单位Bagent1.sinks.des.hdfs.rollSize = 100000#hdfs有多少条消息记录时，创建文件，0为不基于条数判断agent1.sinks.des.hdfs.rollCount = 10000#hdfs空闲多久就新建一个文件,单位秒agent1.sinks.des.hdfs.idleTimeout = 30##下面是把上面设置的组件关联起来（把点用线连起来）agent1.sources.src.channels = chagent1.sinks.des.channel    = ch</code></pre><p>启动flume命令：</p><p><code>flume-ng agent -c /conf -f /apps/flume/conf/exec_mem_hdfs.conf -n agent1 -Dflume.root.logger=DEBUG,console</code></p><p>在另一窗口，查看HDFS上的输出<br><code>hadoop fs -ls -R /myflume2</code><br><img src="https://img-blog.csdnimg.cn/20190918202648957.png" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;为什么用Flume：&quot;&gt;&lt;a href=&quot;#为什么用Flume：&quot; class=&quot;headerlink&quot; title=&quot;为什么用Flume：&quot;&gt;&lt;/a&gt;为什么用Flume：&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume多source，多sink组合框架搭建</title>
    <link href="http://yoursite.com/2019/09/29/Flume%E5%A4%9Asource%EF%BC%8C%E5%A4%9Asink%E7%BB%84%E5%90%88%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/09/29/Flume多source，多sink组合框架搭建/</id>
    <published>2019-09-29T15:08:01.000Z</published>
    <updated>2019-09-29T15:08:18.134Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Flume多source，多sink组合框架搭建"><a href="#Flume多source，多sink组合框架搭建" class="headerlink" title="Flume多source，多sink组合框架搭建"></a>Flume多source，多sink组合框架搭建</h2><p>Flume运行的核心是Agent。它是一个完整的数据收集工具，含有三个核心组件，分别是Source、Channel、Sink。通过这些组件，Event可以从一个地方流向另一个地方。</p><p>Source可以接收外部源发送过来的数据。不同的Source可以接受不同的数据格式。</p><p>Channel是一个存储地，接收Source的输出，直到有Sink消费掉Channel中的数据。</p><p>Sink消费Channel中的数据，将数据推送给外部源或者其他Source。当Sink写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。</p><p>在实际生产环境中，Flume允许多个Agent连在一起，形成前后相连的多级跳。Flume有多种组合方式。比如多个Source收集不同格式的数据输出到同一个Sink中，或者一个Source收集的数据输出到多个Sink中去。</p><p>现在有三台机器，分别是：Hadoop1，Hadoop2，Hadoop3，以Hadoop1为日志汇总</p><p><img src="https://img-blog.csdnimg.cn/20190918202103648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>Hadoop1汇总的同时往多个目标进行输出<br><img src="https://img-blog.csdnimg.cn/20190918202118491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><code>syslog_mem_hdfsandlogger.conf</code>文件</p><pre><code>#定义各个组件agent1.sources  = srcagent1.channels = ch1 ch2agent1.sinks    = des1 des2#配置sourceagent1.sources.src.type = syslogtcpagent1.sources.src.bind = localhostagent1.sources.src.port = 6868#配置channelagent1.channels.ch1.type = memoryagent1.channels.ch2.type = memory#配置hdfs sinkagent1.sinks.des1.type = hdfsagent1.sinks.des1.hdfs.path = hdfs://localhost:9000/myflume4/syslog_mem_hdfsandlogger/agent1.sinks.des1.hdfs.useLocalTimeStamp = true#设置flume临时文件的前缀为 . 或 _ 在hive加载时，会忽略此文件。agent1.sinks.des1.hdfs.inUsePrefix=_#设置flume写入文件的前缀是什么agent1.sinks.des1.hdfs.filePrefix = q7agent1.sinks.des1.hdfs.fileType = DataStreamagent1.sinks.des1.hdfs.writeFormat = Text#hdfs创建多久会新建一个文件，0为不基于时间判断,单位为秒agent1.sinks.des1.hdfs.rollInterval = 20#hdfs写入的文件达到多大时，创建新文件 0为不基于空间大小,单位Bagent1.sinks.des1.hdfs.rollSize = 10#hdfs有多少条消息记录时，创建文件，0为不基于条数判断agent1.sinks.des1.hdfs.rollCount = 5#hdfs空闲多久就新建一个文件,单位秒agent1.sinks.des1.hdfs.idleTimeout = 20#配置logger sinkagent1.sinks.des2.type = logger</code></pre><p>.启动Flume，执行收集工作</p><pre><code>cd /apps/flume  flume-ng agent -c /conf -f /apps/flume/conf/syslog_mem_hdfsandlogger.conf -n agent1 -Dflume.root.logger=DEBUG,console  </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Flume多source，多sink组合框架搭建&quot;&gt;&lt;a href=&quot;#Flume多source，多sink组合框架搭建&quot; class=&quot;headerlink&quot; title=&quot;Flume多source，多sink组合框架搭建&quot;&gt;&lt;/a&gt;Flume多source，多
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Flume传输数据给Kafka</title>
    <link href="http://yoursite.com/2019/09/29/Flume%E4%BC%A0%E8%BE%93%E6%95%B0%E6%8D%AE%E7%BB%99Kafka/"/>
    <id>http://yoursite.com/2019/09/29/Flume传输数据给Kafka/</id>
    <published>2019-09-29T15:07:18.000Z</published>
    <updated>2019-09-29T15:07:48.218Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><p>Flume是一个海量日志采集、聚合和传输的日志收集系统。</p><p>Kafka是一个可持久化的分布式的消息队列。</p><p>由于采集和处理数据的速度不一定同步，所以使用Kafka这个消息中间件来缓冲，如果你收集了日志后，想输出到多个业务方也可结合Kafka，Kafka支持多个业务来读取数据。<img src="https://img-blog.csdnimg.cn/20190918201857107.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上图中Kafka生产的数据，是由Flume提供的，这里我们需要用到Flume集群，通过Flume集群将Agent的日志收集分发到Kafka（供实时计算处理）和HDFS（离线计算处理）。<br><img src="https://img-blog.csdnimg.cn/20190918201918836.png" alt="在这里插入图片描述"><br>Flume将收集到的数据输送到Kafka中间件，以供Storm去实时消费计算，整个流程从各个Web节点上，通过Flume的Agent代理收集日志，然后汇总到Flume集群，再由Flume的Sink将日志输送到Kafka集群，完成数据的传输流程。</p><pre><code>#定义各个组件  agent1.sources  = src  agent1.channels = ch_hdfs ch_kafka  agent1.sinks    = des_hdfs des_kafka  #配置source  agent1.sources.src.type = syslogtcp  agent1.sources.src.bind = localhost  agent1.sources.src.port = 6666  #配置channel  agent1.channels.ch_hdfs.type = memory  agent1.channels.ch_kafka.type = memory  #配置hdfs sink  agent1.sinks.des_hdfs.type = hdfs  agent1.sinks.des_hdfs.hdfs.path = hdfs://localhost:9000/myflume/syslog_mem_hdfsandkafka/  agent1.sinks.des_hdfs.hdfs.useLocalTimeStamp = true  #设置flume临时文件的前缀为 . 或 _ 在hive加载时，会忽略此文件。  agent1.sinks.des_hdfs.hdfs.inUsePrefix=_  #设置flume写入文件的前缀是什么  agent1.sinks.des_hdfs.hdfs.filePrefix = q7  agent1.sinks.des_hdfs.hdfs.fileType = DataStream  agent1.sinks.des_hdfs.hdfs.writeFormat = Text  #hdfs创建多久会新建一个文件，0为不基于时间判断,单位为秒  agent1.sinks.des_hdfs.hdfs.rollInterval = 20  #hdfs写入的文件达到多大时，创建新文件 0为不基于空间大小,单位B  agent1.sinks.des_hdfs.hdfs.rollSize = 10  #hdfs有多少条消息记录时，创建文件，0为不基于条数判断  agent1.sinks.des_hdfs.hdfs.rollCount = 5  #hdfs空闲多久就新建一个文件,单位秒  agent1.sinks.des_hdfs.hdfs.idleTimeout = 20  #配置kafka sink  agent1.sinks.des_kafka.type = org.apache.flume.sink.kafka.KafkaSink  agent1.sinks.des_kafka.brokerList = localhost:9092  agent1.sinks.des_kafka.topic = flumekafka  agent1.sinks.des_kafka.batchSize=100  agent1.sinks.des_kafka.requiredAcks=1  ##下面是把上面设置的组件关联起来（把点用线连起来）  agent1.sources.src.channels = ch_hdfs ch_kafka  agent1.sinks.des_hdfs.channel    = ch_hdfs  agent1.sinks.des_kafka.channel   = ch_kafka  </code></pre><p>启动kafka-server</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Flume&quot;&gt;&lt;a href=&quot;#Flume&quot; class=&quot;headerlink&quot; title=&quot;Flume&quot;&gt;&lt;/a&gt;Flume&lt;/h2&gt;&lt;p&gt;Flume是一个海量日志采集、聚合和传输的日志收集系统。&lt;/p&gt;
&lt;p&gt;Kafka是一个可持久化的分布式的消息队列。
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
      <category term="Kafka" scheme="http://yoursite.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习笔记</title>
    <link href="http://yoursite.com/2019/09/29/Hive%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/09/29/Hive学习笔记/</id>
    <published>2019-09-29T15:06:44.000Z</published>
    <updated>2019-09-29T15:06:59.991Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive内部是什么"><a href="#Hive内部是什么" class="headerlink" title="Hive内部是什么"></a>Hive内部是什么</h1><p>Hive二进制分支版本核心包含3个部分。主要部分是Java代码本身。在$HIVE_HOME/lib 目录下发现有众多的jar包文件。</p><p>所有的Hive客户端都需要一个metastoreservice（元数据服务），Hive使用这个服务来存储表模式信息和其他元数据信息。通常情况下会使用一个关系型数据库中的表来存储这些信息。默认情况下，Hive会使用内置的Derby sql服务器，  我这里使用的是MySQL</p><p>最后，Hive还提供了一个简单的网页界面，也就是Hive网页界面，提供了远程访问的Hive服务</p><p>conf目录下存放了配置Hive的配置文件。Hive具有非常多的配置属性，根据需要后面我们会进行介绍。</p><h2 id="启动Hive"><a href="#启动Hive" class="headerlink" title="启动Hive"></a>启动Hive</h2><p>在bin目录下 运行<br><code>./hive</code><br>字符串hive&gt;是Hive的提示符：</p><h3 id="使用JDBC连接元数据"><a href="#使用JDBC连接元数据" class="headerlink" title="使用JDBC连接元数据"></a>使用JDBC连接元数据</h3><p>Hive所需要的组件中只有一个外部组件是Hadoop没有的，那就是metastore（元数据存储）组件。元数据存储中存储了如表的模式和分区信息等元数据信息。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。</p><h2 id="为什么使用Hive"><a href="#为什么使用Hive" class="headerlink" title="为什么使用Hive"></a>为什么使用Hive</h2><ul><li><p>直接使用Hadoop所面临的问题<br>– 学习成本高<br>– 一般项目周期要求短<br>– MapReduce实现复杂的查询呢逻辑开发难度较大</p></li><li><p>为什么要使用Hive<br>– 操作接口采用类SQL语法，可以快速开发<br>– 避免编写MR，减少学习成本<br>– 扩展功能很方便</p></li></ul><p>Hive的特点</p><ul><li>可扩展： Hive可以自由的扩展集群规模，一般情况下不需要重启服务</li><li>延展性： Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li><li>容错： 良好的容错性，节点出现问题SQL仍然可以完成执行<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><img src="https://img-blog.csdnimg.cn/20190918195755411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>JobTracker是Hadoop1.x中的ResouceManager + AppMaster</li></ul><p>TaskTracker相当于NodeManager + YarnChild</p><h3 id="基本组成"><a href="#基本组成" class="headerlink" title="基本组成"></a>基本组成</h3><ul><li>用户接口：CLI、JDBC/ODBC、WebGUI<br>– CLI是SHELL命令行<br>– JDBC、ODBC是Hive的Java实现，与传统的JDBC类似<br>– WebGUI是通过浏览器访问Hive</li><li>元数据存储：Mysql、Derby等<br>– Hive将元数据存储在数据库中。Hive中的元数据包括表名、列、分区及其属性、表的属性（是否为外部表等）、表数据所在目录等。</li><li>解释器、编译器、优化器、执行器<br>– 完成HQL查询语句 词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后由MapReduce调用执行。<h3 id="Hive与Hadoop的关系"><a href="#Hive与Hadoop的关系" class="headerlink" title="Hive与Hadoop的关系"></a>Hive与Hadoop的关系</h3><img src="https://img-blog.csdnimg.cn/20190918195918115.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><h3 id="Hive与传统数据库对比"><a href="#Hive与传统数据库对比" class="headerlink" title="Hive与传统数据库对比"></a>Hive与传统数据库对比</h3>Hive具有SQL数据库的外表，但应用应用场景完全不同，Hive只适合用来做批量的数据统计分析</li></ul><p><img src="https://img-blog.csdnimg.cn/20190918200059180.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="Hive的数据存储"><a href="#Hive的数据存储" class="headerlink" title="Hive的数据存储"></a>Hive的数据存储</h3><p>Hive中所有的数据都存储在HDFS中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等）</p><p>只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。</p><p>Hive中包含以下数据模型：DB、Table、External Table、Partition、Bucket</p><pre><code>DB：在HDFS中表现为 ${hive.metastore.warehouse.dir} 目录下的文件夹Table：在HDFS中表现所属DB目录下的文件夹External Table：外部表，与Table类似，不过其数据存放的位置可以在任意指定路径。Table表：删除表后，HDFS上的文件都删除了External外部表：删除表后，HDFS上的文件没有删除，只是把表删除了Partition：在HDFS中表现为Table目录下的子目录Bucket：桶，在HDFS中表现为同一个表目录下根据Hash散列之后的多个文件，会根据不同的文件把数据放到不同的文件夹中</code></pre><h2 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h2><h3 id="DDL操作"><a href="#DDL操作" class="headerlink" title="DDL操作"></a>DDL操作</h3><h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><p>建表语法:</p><pre><code>CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name     [(col_name data_type [COMMENT col_comment], ...)]     [COMMENT table_comment]    [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]    [CLUSTERED BY (col_name, col_name, ...)    [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]    [ROW FORMAT row_format]    [STORED AS file_format]    [LOCATION hdfs_path]</code></pre><p>说明：</p><p><code>CREATE TABLE</code> 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p><p><code>EXTERNAL</code>关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p><p><code>PARTITIONED</code> 表示根据某一个key(不在create table里面)对数据进行分区，体现在HDFS上就是 table目录下有n个不同的分区文件夹(country=China,country=USA)</p><p><code>ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char][MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]| SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</code></p><p>用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。</p><p><code>STORED AS</code><br>SEQUENCEFILE|TEXTFILE|RCFILE<br>如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p><p><code>CLUSTERED BY</code><br>对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。<br>把表（或者分区）组织成桶（Bucket）有两个理由：</p><p>（1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。<br>（2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。</p><pre><code>show databases;use hive_test_db;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) sorted by(Sno DESC)into 4 bucketsrow format delimitedfields terminated by &apos;,&apos;;</code></pre><p>设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数</p><pre><code>set hive.enforce.bucketing = true;set mapreduce.job.reduces=4;</code></pre><h4 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h4><p>增加/删除分区</p><p><code>ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &#39;location1&#39; ] partition_spec [ LOCATION &#39;location2&#39; ] ...partition_spec:: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)ALTER TABLE table_name DROP partition_spec, partition_spec,...</code></p><p>实例：</p><p><code>alter table student_p add partition(part=&#39;a&#39;) partition(part=&#39;b&#39;);</code></p><p>重命名表</p><p><code>ALTER TABLE table_name RENAME TO new_table_name</code></p><p>增加/更新列</p><p><code>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive内部是什么&quot;&gt;&lt;a href=&quot;#Hive内部是什么&quot; class=&quot;headerlink&quot; title=&quot;Hive内部是什么&quot;&gt;&lt;/a&gt;Hive内部是什么&lt;/h1&gt;&lt;p&gt;Hive二进制分支版本核心包含3个部分。主要部分是Java代码本身。在$HIVE_H
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive beeline Hiveserver2</title>
    <link href="http://yoursite.com/2019/09/29/Hive-beeline-Hiveserver2/"/>
    <id>http://yoursite.com/2019/09/29/Hive-beeline-Hiveserver2/</id>
    <published>2019-09-29T15:05:51.000Z</published>
    <updated>2019-09-29T15:06:20.153Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HiveServer2"><a href="#HiveServer2" class="headerlink" title="HiveServer2"></a>HiveServer2</h2><p>HiveServer 2(HS2)是一种服务，使客户端能够对Hive执行查询。HiveServer 2是HiveServer 1的继承者，HiveServer 1已被废弃。HS2支持多客户端并发和身份验证。它的设计是为了更好地支持开放API客户机，如JDBC和ODBC。<strong>HS2是一个作为复合服务运行的单个进程</strong>，它包括基于<strong>Thwift的Hive服务</strong>(TCP或HTTP)和用于WebUI的<strong>JettyWeb</strong>服务器。</p><h2 id="启动HiveServer2"><a href="#启动HiveServer2" class="headerlink" title="启动HiveServer2"></a>启动HiveServer2</h2><p>在bin目录下</p><p><code>./hiveserver2</code></p><p>启动hive的服务：</p><p><code>[root@hdp20-04 hive-1.2.1]# bin/hiveserver2 -hiveconf hive.root.logger=DEBUG,console</code></p><p>上述启动，会将这个服务启动在前台，如果要启动在后台，则命令如下：</p><p><code>nohup bin/hiveserver2 1&gt;/dev/null 2&gt;&amp;1 &amp;</code></p><pre><code>&amp; 后台输出1,标准输出2,错误输出/dev/null ,Linux中的黑洞,表示不存储信息；nohup,用户即使退出，程序也会在后台运行</code></pre><p><img src="https://img-blog.csdnimg.cn/20190918190439748.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="使用beeline连接hive2时报错"><a href="#使用beeline连接hive2时报错" class="headerlink" title="使用beeline连接hive2时报错"></a>使用beeline连接hive2时报错</h3><p><code>Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000/default: java.net.ConnectException: 拒绝连接 (Connection refused) (state=08S01,code=0)</code></p><p>原因：<br><img src="https://img-blog.csdnimg.cn/20190918195203780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>解决方法：<br>对<code>hadoop的core-site.xml</code>进行配置，在文件里面添加并保存</p><pre><code>&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;&lt;value&gt;*&lt;/value&gt;&lt;/property&gt;</code></pre><p>重启hadoop</p><h4 id="启动hiveserver2"><a href="#启动hiveserver2" class="headerlink" title="启动hiveserver2"></a>启动hiveserver2</h4><p> <code>hiveserver2</code></p><p>查看hiveserver2启动状态</p><p> <code>netstat -nptl | grep 10000</code></p><p> 接着启动beeline</p><p><code>beeline</code></p><p>最后使用jdbc连接数据库</p><p><code>!connect jdbc:hive2://master:10000/default</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;HiveServer2&quot;&gt;&lt;a href=&quot;#HiveServer2&quot; class=&quot;headerlink&quot; title=&quot;HiveServer2&quot;&gt;&lt;/a&gt;HiveServer2&lt;/h2&gt;&lt;p&gt;HiveServer 2(HS2)是一种服务，使客户端能够对Hive
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive报错记录</title>
    <link href="http://yoursite.com/2019/09/29/Hive%E6%8A%A5%E9%94%99%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/09/29/Hive报错记录/</id>
    <published>2019-09-29T15:05:24.000Z</published>
    <updated>2019-09-29T15:05:36.986Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20190918185405392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>创建表的时候报错，重新复制了一下操作成功</p><hr><p>使用load函数传数据的时候报错<br><img src="https://img-blog.csdnimg.cn/2019091818545314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>问题出在用户夹错误<img src="https://img-blog.csdnimg.cn/20190918185512699.png" alt="在这里插入图片描述"><br>修改配置文件  <code>hive-site,xml</code></p><pre><code>&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;#hive元数据存放目录,hdfs&lt;value&gt;/usr/hive/warehouse&lt;/value&gt;&lt;name&gt;hive.exec.scratchdir&lt;/name&gt;#hive缓存存放目录,hdfs&lt;value&gt;/tmp/hive&lt;/value&gt;    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;#hive缓存存放目录,客户端&lt;value&gt;/root/hive/tmp&lt;/value&gt;&lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;#资源下载目录,客户端&lt;value&gt;/root/hive/tmp&lt;/value&gt;&lt;name&gt;hive.querylog.location&lt;/name&gt;#hive查询日志路径,客户端&lt;value&gt;/root/hive/logs&lt;/value&gt;&lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;#hive日志存放目录,客户端&lt;value&gt;/root/hive/logs&lt;/value&gt;&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;#设置hive环境下默认显示当前所在数据库名称&lt;value&gt;true&lt;/value&gt;&lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt;#设置hive环境下优先使用本地hadoop执行MR操作,节省时间&lt;value&gt;true&lt;/value&gt;&lt;name&gt;hive.cli.print.header&lt;/name&gt;#设置hive环境查询数据表时显示列名&lt;value&gt;true&lt;/value&gt;&lt;name&gt;hive.mapred.mode&lt;/name&gt;#设置查询模式为非严格,默认无法实现笛卡尔积查询&lt;value&gt;nonstrict&lt;/value&gt;</code></pre><p>修改完 初始化 继续报错<br><img src="https://img-blog.csdnimg.cn/2019091818555427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>原因：在初始化之前hive数据库需要删除<br><img src="https://img-blog.csdnimg.cn/20190918185610405.png" alt="在这里插入图片描述"><br>删除hive数据库：<br><code>drop database hive;</code></p><p>重新初始化 成功！！！</p><hr><p>运行load函数时</p><p><code>load data local inpath &#39;/root/emp.txt&#39; into table t_emp;</code><br><img src="https://img-blog.csdnimg.cn/20190918185656956.png" alt="在这里插入图片描述"><br>原因：</p><p>slave1 与slave2 防火墙没关！！！！！</p><p>master当然也必须要关</p><p>在slave1与slave2运行以下命令</p><pre><code>systemctl stop firewalldsetenforce 0#临时关闭selinux模式 setenforce 0</code></pre><p>查看防火墙状态：</p><p><code>systemctl status firewalld</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20190918185405392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive2.3.0的环境搭建</title>
    <link href="http://yoursite.com/2019/09/29/Hive2-3-0%E7%9A%84%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/09/29/Hive2-3-0的环境搭建/</id>
    <published>2019-09-29T15:04:27.000Z</published>
    <updated>2019-09-29T15:05:00.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><p><strong>完成hadoop的安装</strong><br><strong>完成mysql的安装</strong></p><h2 id="下载Hive"><a href="#下载Hive" class="headerlink" title="下载Hive"></a>下载Hive</h2><p><code>wget http://mirror.bit.edu.cn/apache/hive/hive-2.3.0/apache-hive-2.3.0-bin.tar.gz</code><br>或者去官网安装</p><h3 id="解压到指定安装目录"><a href="#解压到指定安装目录" class="headerlink" title="解压到指定安装目录"></a>解压到指定安装目录</h3><p>用xftp将安装包传到opt/hadoop中</p><p>解压：<br><code>tar -zxvf apache-hive-2.3.0-bin.tar.gz</code></p><p>修改文件夹名称：<br><code>mv ./apache-hive-2.3.0-bin ./hive-2.3.0</code></p><h3 id="修改环境变量"><a href="#修改环境变量" class="headerlink" title="修改环境变量"></a>修改环境变量</h3><p><code>vi /etc/profile</code></p><p>插入</p><pre><code>export HIVE_HOME=/opt/hadoop/hive-2.3.0export PATH=$HIVE_HOME/bin:$PATH</code></pre><p>使其修改立即生效<br><code>source /etc/profile</code></p><h3 id="登录mysql数据库"><a href="#登录mysql数据库" class="headerlink" title="登录mysql数据库"></a>登录mysql数据库</h3><p>并创建metastore数据库，关闭新主库的只读属性，为其授权（用于存储hive的初始化配置）</p><p><code>create database metastore;</code></p><pre><code>set global read_only=0;grant all on metastore.* to hive@&apos;%&apos;  identified by &apos;hive&apos;;grant all on metastore.* to hive@&apos;localhost&apos;  identified by &apos;hive&apos;;</code></pre><p><code>flush privileges;</code></p><p><img src="https://img-blog.csdnimg.cn/20190918183324400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>如果不关闭数据库的只读属性，执行</strong><br><code>grant all on metastore.* to hive@&#39;%&#39;  identified by &#39;hive&#39;;</code>时，会报错</p><h3 id="下载jdbc-connector"><a href="#下载jdbc-connector" class="headerlink" title="下载jdbc connector"></a>下载jdbc connector</h3><p>点击链接<a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">Connector/J 5.1.44</a>下载至本地主机，然后再传至</p><p><code>/opt/hadoop/hive-2.3.0/lib</code><br><img src="https://img-blog.csdnimg.cn/20190918183455830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="修改hive配置文件"><a href="#修改hive配置文件" class="headerlink" title="修改hive配置文件"></a>修改hive配置文件</h3><p><code>cd /opt/hadoop/hive-2.3.0/conf/</code></p><p>重命名配置文件</p><pre><code>cp hive-env.sh.template hive-env.shcp hive-default.xml.template hive-site.xmlcp hive-log4j2.properties.template hive-log4j2.propertiescp hive-exec-log4j2.properties.template hive-exec-log4j2.properties</code></pre><p>修改<code>hive-env.sh</code>文件</p><pre><code>export JAVA_HOME=/opt/java/jdk1.8.0_201    ##Java路径,根据自己jdk安装的路径配置export HADOOP_HOME=/opt/hadoop/hadoop-2.8.0   ##Hadoop安装路径export HIVE_HOME=/opt/hadoop/hive-2.3.0    ##Hive安装路径export HIVE_CONF_DIR=/opt/hadoop/hive-2.3.0/conf   ##Hive配置文件路径</code></pre><hr><pre><code>hadoop   fs   -mkdir   -p   /usr/hive/warehousehadoop   fs   -chmod   777   /usr/hive/warehouse hadoop   fs   -mkdir  -p   /tmp/hive/hadoop   fs   -chmod  777   /tmp/hivehadoop   fs   -ls   /usr/hive/hadoop   fs   -ls   /tmp/</code></pre><h4 id="修改hive-site-xml中的临时目录"><a href="#修改hive-site-xml中的临时目录" class="headerlink" title="修改hive-site.xml中的临时目录"></a>修改hive-site.xml中的临时目录</h4><p>将hive-site.xml文件中的${system:java.io.tmpdir}替换为hive的临时目录，例如我替换为/root/hive/tmp，该目录如果不存在则要自己手工创建，并且赋予读写权限。</p><p><strong>将${system:user.name}都替换为root</strong></p><h4 id="修改hive-site-xml数据库相关的配置"><a href="#修改hive-site-xml数据库相关的配置" class="headerlink" title="修改hive-site.xml数据库相关的配置"></a>修改hive-site.xml数据库相关的配置</h4><p>搜索<code>javax.jdo.option.ConnectionURL</code>，将该name对应的value修改为MySQL的地址，例如我修改后是：</p><pre><code>&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;  &lt;value&gt;jdbc:mysql://172.18.74.236:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</code></pre><p>搜索<code>javax.jdo.option.ConnectionDriverName</code>，将该name对应的value修改为MySQL驱动类路径，例如我的修改后是：</p><pre><code> &lt;property&gt;        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt;      </code></pre><p>  搜索<code>javax.jdo.option.ConnectionUserName</code>，将对应的value修改为MySQL数据库登录名：</p><pre><code>&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;</code></pre><p> 搜索javax.jdo.option.ConnectionPassword，将对应的value修改为MySQL数据库的登录密码：</p><pre><code>&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;******&lt;/value&gt;</code></pre><p>修改为自己的密码</p><p>搜索<code>hive.metastore.schema.verification</code>，将对应的value修改为false：</p><pre><code>&lt;name&gt;hive.metastore.schema.verification&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;</code></pre><h2 id="启动和测试"><a href="#启动和测试" class="headerlink" title="启动和测试"></a>启动和测试</h2><h2 id="对MySQL数据库进行初始化"><a href="#对MySQL数据库进行初始化" class="headerlink" title="对MySQL数据库进行初始化"></a>对MySQL数据库进行初始化</h2><p>进入到hive的bin目录 执行命令：</p><p><code>schematool   -initSchema  -dbType  mysql</code></p><p><img src="https://img-blog.csdnimg.cn/20190918184303471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>报出两个错：</strong></p><p><strong>第一个</strong>：</p><pre><code>Fri Mar 30 14:55:35 CST 2018 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</code></pre><p>仔细看了看，发现是字符串中没有配置SSL这个配置项。</p><p>需要在连接字符串中加上<code>useSSL=false/true</code>配置。</p><pre><code>&lt;value&gt;jdbc:mysql://172.18.74.236:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</code></pre><p>修改为</p><pre><code>&lt;value&gt;jdbc:mysql://172.18.74.236:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt;</code></pre><p><strong>第二个：</strong></p><p>在安装Hive的时候报错：</p><pre><code>org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version.Underlying cause: java.sql.SQLException : Access denied for user &apos;root&apos;@&apos;master.hadoop&apos; (using password: YES)</code></pre><p>解决方案：</p><ol><li><p>首先登陆用root用户登录mysql<br><code>mysql -u root -p</code></p></li><li><p>查看root下<br><code>mysql&gt; select user,host from mysql.user where user=&#39;root&#39;;</code><br><img src="https://img-blog.csdnimg.cn/20190918184536558.png" alt="在这里插入图片描述"><br>原因是‘root‘@’master’用户权限不足，利用root用户给该用户分配权限<br><code>mysql&gt; grant all on *.* to &#39;root&#39;@&#39;master&#39; identified by &#39;你的数据库密码&#39;</code><br><img src="https://img-blog.csdnimg.cn/20190918184622348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>退出 重新初始化<br>成功！<br><img src="https://img-blog.csdnimg.cn/20190918184647644.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前期准备&quot;&gt;&lt;a href=&quot;#前期准备&quot; class=&quot;headerlink&quot; title=&quot;前期准备&quot;&gt;&lt;/a&gt;前期准备&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;完成hadoop的安装&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;完成mysql的安装&lt;/strong&gt;&lt;/p
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Spark 2.4.2的环境搭建</title>
    <link href="http://yoursite.com/2019/09/29/Spark-2-4-2%E7%9A%84%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2019/09/29/Spark-2-4-2的环境搭建/</id>
    <published>2019-09-29T15:03:17.000Z</published>
    <updated>2019-09-29T15:06:25.872Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>依赖环境：Scala</p></blockquote><p>Spark是使用Scala编写的，用Scala编写Spark任务可以像操作本地集合对象一样操作分布式数据集RDD</p><p>安装的过程可以参考我的这篇文章<a href="https://plutoacharon.github.io/2019/08/08/Scala%E7%9A%84%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">Scala安装</a></p><h2 id="安装完scala可以查看版本"><a href="#安装完scala可以查看版本" class="headerlink" title="安装完scala可以查看版本"></a>安装完scala可以查看版本</h2><p><code>scala -version</code></p><p>这里重点介绍Spark的安装，相比于hadoop的安装要简单一些，而且步骤类似，话不多说，开始！</p><h2 id="Spark的安装"><a href="#Spark的安装" class="headerlink" title="Spark的安装"></a>Spark的安装</h2><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p><code>tar -zxvf /spark-2.2.2-bin-hadoop2.7.tgz</code><br><code>mv spark-2.4.2-bin-hadoop2.7 spark-2.4.2</code></p><h3 id="修改环境变量"><a href="#修改环境变量" class="headerlink" title="修改环境变量"></a>修改环境变量</h3><pre><code>#sparkexport SPARK_HOME=/opt/hadoop/spark-2.4.2export PATH=$PATH:$SPARK_HOME/bin</code></pre><h3 id="重新加载环境"><a href="#重新加载环境" class="headerlink" title="重新加载环境"></a>重新加载环境</h3><pre><code>source /etc/profile</code></pre><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p><code>mv spark-env.sh.template spark-env.sh</code></p><p><code>vim spark-env.sh</code></p><pre><code>export SPARK_MASTER_IP=masterexport SCALA_HOME=/opt/hadoop/scala-2.12.8export SPARK_WORKER_MEMORY=6gexport JAVA_HOME=/opt/java/jdk1.8.0_201export HADOOP_HOME=/opt/hadoop/hadoop-2.8.0/export HADOOP_CONF_DIR=/opt/hadoop/hadoop-2.8.0/etc/hadoop</code></pre><p>配置spark从节点，修改slaves文件</p><p><code>cp slaves.template slaves</code></p><p>修改为</p><pre><code>slave1slave2</code></pre><p>将文件传到两个从节点上</p><pre><code>scp -r spark-2.4.2 root@slave1:/opt/hadoop/scp -r spark-2.4.2 root@slave2:/opt/hadoop/</code></pre><p>在将salve1与slave2的spark环境添加即可</p><p>在bin目录下输入  </p><p><code>./spark-shell</code></p><p>可进入脚本环境</p><p>在浏览器输入<code>IP地址:8080</code><br><img src="https://img-blog.csdnimg.cn/20190918155253813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="Spark目录"><a href="#Spark目录" class="headerlink" title="Spark目录"></a>Spark目录</h2><p>bin包含用来和Spark交互的可执行文件，如Spark shell。</p><p>core，Streaming，python，…包含主要组件的源代码。</p><p>example包含一些单机SparkJob，你可以研究和运行这些例子。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;依赖环境：Scala&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Spark是使用Scala编写的，用Scala编写Spark任务可以像操作本地集合对象一样操作分布式数据集RDD&lt;/p&gt;
&lt;p&gt;安装的过程可以参考我的这篇文章&lt;a href=&quot;http
      
    
    </summary>
    
      <category term="Spark" scheme="http://yoursite.com/categories/Spark/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>VMware Workstation 14运行虚拟机黑屏时的解决方案</title>
    <link href="http://yoursite.com/2019/09/29/VMware-Workstation-14%E8%BF%90%E8%A1%8C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%BB%91%E5%B1%8F%E6%97%B6%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>http://yoursite.com/2019/09/29/VMware-Workstation-14运行虚拟机黑屏时的解决方案/</id>
    <published>2019-09-29T15:00:38.000Z</published>
    <updated>2019-09-29T15:02:59.034Z</updated>
    
    <content type="html"><![CDATA[<p>管理员运行 <code>netsh winsock reset</code> 帮你解决一切烦恼</p><p>好吧，说正经的，虚拟机和主机之间的通信，基本上是以 socket 的方式进行通信的（这里的 socket 泛指一切 socket，包括本地的、网络的等等）</p><p>某个程序通过 LSP 给系统的 TCP/IP stack 注入了自己的 DLL，如果程序退出时，没有把这个 DLL 收回来，或者回收失败、没有彻底回收等，那么这个 DLL 残留的东西影响了整个 TCP/IP stack，而且是永久性的</p><p>每个 socket 收发的数据包是要经过 TCP/IP stack 的，这个 DLL 可能会对这些 socket 的数据包做了些什么操作（比如修改数据、直接丢弃等等），然后所以你的虚拟机就黑屏了……</p><p><code>netsh winsock reset</code>这条命令会重置，所以那些被注入的 DLL 就被清理掉了，所以一切都是最初的模样了……</p><h2 id="不用重启的方法"><a href="#不用重启的方法" class="headerlink" title="不用重启的方法"></a>不用重启的方法</h2><p>关闭虚拟机cmd管理员权限运行以下命令</p><pre><code>netsh winsock resetnet stop VMAuthdServicenet start VMAuthdServicenet stop VMwareHostdnet start VMwareHostd</code></pre><p>运行完成后即可使用，基本不用重启</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;管理员运行 &lt;code&gt;netsh winsock reset&lt;/code&gt; 帮你解决一切烦恼&lt;/p&gt;
&lt;p&gt;好吧，说正经的，虚拟机和主机之间的通信，基本上是以 socket 的方式进行通信的（这里的 socket 泛指一切 socket，包括本地的、网络的等等）&lt;/p&gt;

      
    
    </summary>
    
      <category term="Liunx" scheme="http://yoursite.com/categories/Liunx/"/>
    
    
      <category term="VMware" scheme="http://yoursite.com/tags/VMware/"/>
    
  </entry>
  
  <entry>
    <title>Esxi centos7搭建NAT和DHCP服务器</title>
    <link href="http://yoursite.com/2019/09/29/Esxi-centos7%E6%90%AD%E5%BB%BANAT%E5%92%8CDHCP%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://yoursite.com/2019/09/29/Esxi-centos7搭建NAT和DHCP服务器/</id>
    <published>2019-09-29T14:57:17.000Z</published>
    <updated>2019-09-29T14:59:21.525Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是NAT"><a href="#什么是NAT" class="headerlink" title="什么是NAT"></a>什么是NAT</h2><p>NAT（Network address translation）即网络地址转换，作为一种过渡解决手段，可以用来减少对全球合法IP地址的需求。简单的说，NAT就是在内部专用网络中使用内部地址，而当内部节点要与外界网络发生联系时，就在边缘路由器或者防火墙处，将内部地址转换成公网地址，从而使得在外网（Internet）上使用一个和数个合法IP地址正常传输数据。其中，这里的外网和内网是相对来讲的，下面假设能够访问互联网的网络为外网。</p><h2 id="什么是DHCP"><a href="#什么是DHCP" class="headerlink" title="什么是DHCP"></a>什么是DHCP</h2><p>DHCP（Dynamic Host Configuration Protocol，动态主机配置协议）通常被应用在大型的局域网络环境中，主要作用是集中的管理、分配IP地址，使网络环境中的主机动态的获得IP地址、Gateway地址、DNS服务器地址等信息，并能够提升地址的使用率。</p><h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>vSphere Client远程esxi</p><h3 id="配置网卡"><a href="#配置网卡" class="headerlink" title="配置网卡"></a>配置网卡</h3><p>首先新建一个虚拟交换机</p><p><img src="https://img-blog.csdnimg.cn/2019090719161810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>注意选择创建交换机的时候，不要选中网卡，把默认选中的网卡前的对勾删掉，之所以这么做，是因为我们要把这个网络的请求都转发到VM Network上去，而不要让它自己走物理网卡出去。</p><p><img src="https://img-blog.csdnimg.cn/20190907191645822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>点击下一步给交换机随便起个名字，一路下一步即可</p><p>然后新建一个双网卡的虚拟机，我用的是最小化安装centos7</p><p>然后打开虚拟机设置</p><p><img src="https://img-blog.csdnimg.cn/20190907191704314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190907191721880.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>把网卡2连接到我们刚才新建的虚拟交换机上</p><p>这里记录一下网卡2的mac地址，接下来会用到</p><h3 id="详细配置"><a href="#详细配置" class="headerlink" title="详细配置"></a>详细配置</h3><p>自行配置联网,如果是校园网可以用脚本临时登陆</p><p>更新yum源<br><code>yum -y update</code><br>确保我们yum到的软件都是最新的</p><p><code>yum -y install net-tools</code></p><p>安装一些基本命令</p><p>接下来输入ifconfig找到刚才记录的mac地址，把这个网卡ip设置成内网ip，另一个设置成外网ip</p><p>配置如下，外网ip及网关根据实际情况配置，内网ip可参照互联网协议设置(我这里ens160为外网网卡)</p><p>私有地址的范围分别是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A类地址范围：10.0.0.0—10.255.255.255；</span><br><span class="line">B类地址范围：172.16.0.0---172.31.255.555；</span><br><span class="line">C类地址范围：192.168.0.0---192.168.255.255。</span><br></pre></td></tr></table></figure></p><p><code>vi /etc/sysconfig/network-scripts/ifcfg-ens160</code></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=172.18.74.180</span><br><span class="line">NAME=ens160</span><br><span class="line">DEVICE=ens160</span><br><span class="line">ONBOOT=yes</span><br><span class="line">GATEWAY=172.18.74.253</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line">DNS2=8.8.8.8</span><br></pre></td></tr></table></figure><p><code>vi /etc/sysconfig/network-scripts/ifcfg-ens192</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=192.168.255.1</span><br><span class="line">NAME=ens192</span><br><span class="line">DEVICE=ens192</span><br><span class="line">ONBOOT=yes</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">DNS=114.114.114.114</span><br></pre></td></tr></table></figure></p><p><strong>注意不要写网关</strong></p><p>重启网卡</p><p><code>service network restart</code></p><h4 id="防火墙配置"><a href="#防火墙配置" class="headerlink" title="防火墙配置"></a>防火墙配置</h4><p>关闭防火墙<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></p><p>配置防火墙，让发送至内网网卡(ens192)的数据全部通过</p><p><code>iptables -A FORWARD -i ens192 -j ACCEPT</code></p><p>修改数据报头信息</p><p><code>iptables -t nat -A POSTROUTING -s 192.168.255.0/24 -o ens160 -j MASQUERADE</code></p><p>开启Linux路由功能<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br><span class="line">cat /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure></p><h4 id="安装配置DHCP服务"><a href="#安装配置DHCP服务" class="headerlink" title="安装配置DHCP服务"></a>安装配置DHCP服务</h4><p>安装DHCP服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel*</span><br><span class="line">yum install -y dhcp</span><br></pre></td></tr></table></figure></p><p>复制DHCP配置文件</p><p><code>cp /usr/share/doc/dhcp-4.2.5/dhcpd.conf.example /etc/dhcp/dhcpd.conf</code></p><p>选择yes覆盖原有文件</p><p>编辑并修改dhcpd.conf配置文件</p><p><code>vi /etc/dhcp/dhcpd.conf</code></p><p><img src="https://img-blog.csdnimg.cn/20190907192032136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDQyNTI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>箭头是修改的地方，后面的代码删除即可</p><p>解释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">subnet后跟子网网段</span><br><span class="line">netmask后面是子网掩码</span><br><span class="line">range是地址池范围</span><br><span class="line">option routers是分发默认网关</span><br><span class="line">默认租期时间(秒)</span><br><span class="line">default-lease-time 600；</span><br><span class="line">最大租期时间(秒)</span><br><span class="line">max-lease-time 7200；</span><br><span class="line">DNS服务器地址(多个地址用&quot;,&quot;隔开)</span><br><span class="line">option domain-name-servers 223.5.5.5, 114.114.114.114；</span><br><span class="line">为所分配的域分配域名（名字随便起）</span><br><span class="line">option domain-name &quot;vmnet.com&quot;；</span><br></pre></td></tr></table></figure></p><h3 id="启动DHCP服务"><a href="#启动DHCP服务" class="headerlink" title="启动DHCP服务"></a>启动DHCP服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart dhcpd</span><br><span class="line">systemctl enable dhcpd</span><br></pre></td></tr></table></figure><h3 id="查看启动状态"><a href="#查看启动状态" class="headerlink" title="查看启动状态"></a>查看启动状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl status dhcpd</span><br><span class="line">netstat -uap | grep dhcpd</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20190907192126794.png" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是NAT&quot;&gt;&lt;a href=&quot;#什么是NAT&quot; class=&quot;headerlink&quot; title=&quot;什么是NAT&quot;&gt;&lt;/a&gt;什么是NAT&lt;/h2&gt;&lt;p&gt;NAT（Network address translation）即网络地址转换，作为一种过渡解决手段，可以用
      
    
    </summary>
    
      <category term="Liunx" scheme="http://yoursite.com/categories/Liunx/"/>
    
    
      <category term="Liunx" scheme="http://yoursite.com/tags/Liunx/"/>
    
      <category term="Esxi" scheme="http://yoursite.com/tags/Esxi/"/>
    
  </entry>
  
</feed>
